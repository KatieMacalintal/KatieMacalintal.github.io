[
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Source Code: perceptron.py\nThe perceptron algorithm is a binary classification algorithm. It works to find a hyperplane that splits the given data into their respective labels. This machine learning algorithm, however, has the limitations that it only works on linearly separable data and data separated only into two groups.\n\n\nIn our Perceptron class, we implemented a fit(X, y) algorithm, which finds the variables of weights, w, that linearly separates X, a data set of observations and their features according to their labels y, such that an element in y is either 0 or 1. In this method, we first start with a random value for w. Then while we haven’t reached a 100% classification rate and have not exceeded the maximum number of steps, we continue to tweak our w depending on if it was able to correctly classify a random point i in our data set X.\nThe update of our weights, w, is seen on line 45 of our perceptron.py code. It is equivalent to the following math equation … \\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y}_i {\\langle \\tilde{w}^{(t)}, \\tilde{x}_i\\rangle} < 0)\\tilde{y}_i \\tilde{x}_i\\]\nThis equation only updates w when our prediction for the label of \\(\\tilde{x}_i\\), the randomly selected data point, is incorrect. This is indicated through the \\(\\mathbb{1}(\\tilde{y}_i {\\langle \\tilde{w}^{(t)}, \\tilde{x}_i\\rangle} < 0)\\) portion of the equation, for it will evaulate to 0 when our w does classify the point correctly and the calcuations will not affect \\(\\tilde{w}^{(t+1)}\\), our w in the next iteration. When the indicator evaulates to 1, meaning our predication was incorrect, then our w is incremented by \\(\\tilde{y}_i \\tilde{x}_i\\). This moves our linear separator in the right direction, so that our randomly selected point could be classified correctly on the next iteration if it were selected.\n\n\n\nBefore we conduct any experiments, we need to import and define relevant extensions, classes, and functions.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron\n\nnp.random.seed(12345)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n\n\nHere is a data set of two distinct groups with 2 features.\n\nn = 100\np_features = 2\n\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(-1.75, -1.75), (1.75, 1.75)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAs shown in the graph, it looks like the points could be separated by a line so that all the purple points, whose label would be “0,” are on one side of the line and all the yellow points, whose label would be “1,” are on the other side of the line.\nWhen we run our Perceptron fit method on this data, we can inspect it’s history of scores. We see that over time the score flucuated as w was adjusted, but it eventually reaches 100% classification. This means that the fit method eventually found the variable of weights that would allow it to create a linear separator on the given data.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nWhen we then graph our line using the weights our Perceptron fit method found, we see that our algorithm was indeed able to separate the data into their respective labels.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nHere is another data set of two distinct groups with 2 features.\n\nn = 100\np_features = 2\n\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(0, 0), (0, 0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThis time, however, it does not look like we can separate the purple and yellow points from each other. Since the data points overlap, it does not look like a linear separator could separate the points into their respective labels.\nWhen looking at this set’s history of scores, we see that it too flucates. This time, however, we see that the score never reaches 1, in other words, it does not reach 100% classification. Instead, it eventually reaches the max number of times we’ll adjust w. While the Perceptron fit method still stores the last value of weights in the object, it communicates that it is not accurate as it gives the warning that the data was unable to converge.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n/Users/katiemacalintal/Desktop/Machine Learning/KatieMacalintal.github.io/posts/perceptron/perceptron.py:51: UserWarning: WARNING: Could not converge\n  warnings.warn(\"WARNING: Could not converge\")\n\n\n\n\n\nWhen we then graph our line using the weights the final iteration of our Perceptron fit method found, we see that, as expected, our algorithm was not able to separate the data into their expected labels. (Note: The scale of the axis may also look different)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nOur perceptron algorithm can also work in more than 2 dimensions. This time we’ve created a data set of two distinct groups with 7 features.\n\nn = 100\np_features = 7\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(-1, -1, -1, -1, -1, -1, -1), (1, 1, 1, 1, 1, 1, 1)])\n\nSince this is difficult to visualize, we will only be inspecting its history of scores. As we can see in the chart, our new data set is linearly separable as the score eventually reaches 1.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nWe can also alter this data set, so that the two groups overlap.\n\nn = 100\np_features = 7\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(10, 10, 10, 10, 10, 10, 10), (10, 10, 10, 10, 10, 10, 10)])\n\nAs we can see by inspecting the history of scores and the warning that is thrown, our data set never converges.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n/Users/katiemacalintal/Desktop/Machine Learning/KatieMacalintal.github.io/posts/perceptron/perceptron.py:51: UserWarning: WARNING: Could not converge\n  warnings.warn(\"WARNING: Could not converge\")\n\n\n\n\n\n\n\n\n\nIn the context of a single iteration of the perceptron algorithm update, I think the runtime complexity of our algorithm would depend on \\(p\\), the number of features. The operation that I think take the longest time would be \\({\\langle \\tilde{w}^{(t)}, \\tilde{x}_i\\rangle}\\), which I think takes \\(O(p)\\) time. There is other multiplication and addition that takes place in this equation, but since it happens consecutively it won’t matter at a larger scale. Thus, I think that runtime of the update would be \\(O(p)\\)."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Determine the smallest number of measurements necessary to confidently determine the species of a penguin\n\n\n\n\n\n\nMar 8, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplement first-order methods, including simple gradient descent and stochastic gradient descent with momentum, comparing their performance for training logistic regression\n\n\n\n\n\n\nMar 2, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplement and explain the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets\n\n\n\n\n\n\nFeb 22, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Katie Macalintal\nMiddlebury College\nB.A. Computer Science\n2024"
  },
  {
    "objectID": "posts/optimization/index.html",
    "href": "posts/optimization/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Source Code: logistic-regression.py\nIn this blog post, we use logistic regression and gradient descent to efficiently find a hyperplane that can separate a binary classified data set with minimal loss, in other words minimize the empircal risk.\n\n\nIn our LogisticRegression class, we implemented a fit and fit_stochastic function that both take the data set X and their expected labels y.\nIn the fit function, we are looking for the weights w (which includes the bias term) such that it mimimizes our loss. In order to find this w, we use the gradient descent framework with a convex loss function, which combined allows us to search for this local minima. In this framework, we compute the gradient of our loss function: \\[\\ell(\\hat{y}, y)=-y\\log\\sigma(\\hat{y})-(1-y)\\log(1-\\sigma(\\hat{y})),\\] where \\(\\sigma\\) is the logistic sigmoid function and \\(\\hat{y}\\) is our prediction \\(\\langle w,x_i \\rangle\\). This loss function, known as the logistic loss function, is convient for us because it is strictly convex in it’s first argument meaning that our loss can have at most one minimum. The gradient of this loss function turns out to be: \\[\\nabla L(w)=(1/n)\\sum_{i=1}^n (\\sigma(\\hat{y_i})-y_i)x_i.\\] This gradient equation is implemented in the gradient function of our LogisticRegressions class as\nnp.mean(((self.sigmoid(y_) - y)[:,np.newaxis]) * X, axis = 0).\nThen, as stated in Theorem 2 of Optimization with Gradient Descent notes, because our gradent is a descent direction, we adjust our w by stepping in the direction of descent, since we are looking for a w such that our loss is at the lowest it can be. We do this until we either reach the specified max_epochs or converge. In this case, convergence is until the improvement in the our loss function is small enough in magnitiude.\nThe fit_stochastic function is very similar to the fit function, expect here we don’t compute the complete gradient, we instead compute the gradient on a specified batch size. In this function, there is also the option to specify whether one would like to use momentum.\n\n\n\nBefore we conduct any experiments, we need to import and define relevant extensions, classes, and functions.\n\nfrom logistic_regression import LogisticRegression # source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# Graph fitted line and loss function\ndef graph_fit_and_loss(X, y, LR):\n    fig, axarr = plt.subplots(1, 2)\n\n    axarr[0].scatter(X[:,0], X[:,1], c = y)\n    axarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.loss_history[-1]}\")\n\n    f1 = np.linspace(-3, 3, 101)\n\n    p = axarr[0].plot(f1, (- LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\n    axarr[1].plot(LR.loss_history)\n    axarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n    plt.tight_layout()\n\n\n\nWe first create a set of data with 2 features and see that their labels slightly overlap with each other.\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can then fit this data using our fit method, which uses gradient descent, and fit_stochastic, which used batch gradient descent. When we give these fit methods a reasonable learning rate \\(\\alpha\\), then we would expect them to converge.\n\n# Fit the model\nLR_reg = LogisticRegression()\nLR_reg.fit(X, y, alpha = 0.1, max_epochs = 10000)\n\ngraph_fit_and_loss(X, y, LR_reg)\nplt.savefig('gradient_descent.png')\n\n\n\n\n\n# Fit the model\nLR_stoch = LogisticRegression()\nLR_stoch.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10)\n\ngraph_fit_and_loss(X, y, LR_stoch)\n\n\n\n\n\nnum_steps = len(LR_reg.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_reg.loss_history, label = \"gradient\")\n\nnum_steps = len(LR_stoch.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_stoch.loss_history, label = \"stochastic gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nAfter running these methods, we see that the data was able to converge in both cases. We see that they both reach the same loss, but the stochastic gradient descent reaches it in fewer iterations, meaning it iterated over all data points less times. This may be because it makes updates to w more frequently than regular gradient descent does, since it samples its w on smaller portions of data set X and updates it appropriately.\n\n\n\n\nIt’s important that the learning rate \\(\\alpha\\) is relatively small. Before we set our alpha to 0.1, but if we use the same data and set \\(\\alpha\\) to a too high number we will see that we never converge in both regular gradient descent and stochastic gradient descent. If \\(\\alpha\\) too large, we might be updating w by too much such that it overshoots where the minimum actually is resulting in a flucuating loss history.\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 170, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n/Users/katiemacalintal/Desktop/Machine Learning/KatieMacalintal.github.io/posts/optimization/logistic_regression.py:54: UserWarning: WARNING: Could not converge\n  warnings.warn(\"WARNING: Could not converge\")\n\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 75, max_epochs = 1000, batch_size = 50)\n\ngraph_fit_and_loss(X, y, LR)\n\n/Users/katiemacalintal/Desktop/Machine Learning/KatieMacalintal.github.io/posts/optimization/logistic_regression.py:119: UserWarning: WARNING: Could not converge\n  warnings.warn(\"WARNING: Could not converge\")\n\n\n\n\n\n\nLR.fit(X, y, alpha = 75, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\nThrough graphing the loss history, we see that it fluctuates greatly due to the large \\(\\alpha\\) size. We also see that because stochastic gradient descent updates w more often, our fit_stochastic doesn’t converge with a high learning rate that our regular gradient descent method can actually converge with.\n\n\n\nNow, we will run some experiments on how the batch size influences convergence. We will do this with a larger data set of 1000 points, which have 10 features.\n\n# Make the data\np_features = 11\nX, y = make_blobs(n_samples = 1000, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\ndef graph_loss(LR):\n    fig, axarr = plt.subplots(1, 1)\n\n    axarr.plot(LR.loss_history)\n    axarr.set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = f\"Loss = {LR.loss_history[-1]}\")\n    plt.tight_layout()\n\nIn the Disenroth, Faisal, and Soon reading, they observe that “Large mini-batch sizes will provide accurate estimates of the gradient, reducing the variance in the parameter update. … The reduction in variance leads to more stable convergence, but each graident calculation will be more expensive” (232). On the other hand, they observe that “small mini-batches are quick to estimate. If we keep the mini-batch size small, the noise in our graident estimate will allow us to get out of some bad local optima” (232). We’ll conduct experiments such that our batch_size decreases in size, and we’ll inspect their loss to see how Disenroth, Faisal, and Soon’s observations hold.\n\nLR_500 = LogisticRegression()\nLR_500.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 500)\n\ngraph_loss(LR_500)\n\n\n\n\n\nLR_50 = LogisticRegression()\nLR_50.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 50)\n\ngraph_loss(LR_50)\n\n\n\n\n\nLR_25 = LogisticRegression()\nLR_25.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 25)\n\ngraph_loss(LR_25)\n\n\n\n\n\nLR_5 = LogisticRegression()\nLR_5.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 5)\n\ngraph_loss(LR_5)\n\n\n\n\n\nnum_steps = len(LR_500.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_500.loss_history, label = \"batch size 500\")\n\nnum_steps = len(LR_50.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_50.loss_history, label = \"batch size 50\")\n\nnum_steps = len(LR_25.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_25.loss_history, label = \"batch size 25\")\n\nnum_steps = len(LR_5.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_5.loss_history, label = \"batch size 5\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nWe see that as our batch size gets smaller, our number of iterations over the all data points in X also gets smaller. This relates to why stochastic gradient descent uses fewer iterations as gradient descent. If our batch size is large, we do not update w as frequently as we would have if it were smaller. Based on our loss graphs, we also see that, as noted by Disenroth, Faisal, and Soon, stochastic gradient descent with large batch sizes have a smooth convergence, and small batch sizes have a noiser convergence.\n\n\n\nIn the fit_stochastic function we also implemented the option to use momentum, which takes into account our previously taken step and allows us to continue moving along that direction if the update was good. In the case shown below, we see that momentum was able to result in less iterations over the data, but understanding when exactly momentum would significantly speed up convergence will require more experiments.\n\n# Make the data\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (0, 0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nLR = LogisticRegression()\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = False)\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = True)\ngraph_fit_and_loss(X, y, LR)"
  },
  {
    "objectID": "posts/optimization/momentum_experiments/index.html",
    "href": "posts/optimization/momentum_experiments/index.html",
    "title": "CSCI 0451 Blog",
    "section": "",
    "text": "In this function, we also can use momentum, which takes into account our previously taken step and allows us to continue moving along that direction if the update was good."
  },
  {
    "objectID": "posts/optimization/momentum_experiments/index.html#momentum",
    "href": "posts/optimization/momentum_experiments/index.html#momentum",
    "title": "CSCI 0451 Blog",
    "section": "Momentum",
    "text": "Momentum\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nLR = LogisticRegression()\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = False)\ngraph_fit_and_loss(X, y, LR)\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = True)\ngraph_fit_and_loss(X, y, LR)\n\nDifficult to tell that momentum makes a difference here.\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (0, 0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = False)\ngraph_fit_and_loss(X, y, LR)\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = True)\ngraph_fit_and_loss(X, y, LR)"
  },
  {
    "objectID": "posts/Untitled Folder/index.html",
    "href": "posts/Untitled Folder/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Source Code: logistic-regression.py\nIn this blog post, we use logistic regression and gradient descent to efficiently find a hyperplane that can separate a binary classified data set with minimal loss, in other words minimize the empircal risk.\n\n\nIn our LogisticRegression class, we implemented a fit and fit_stochastic function that both take the data set X and their expected labels y.\nIn the fit function, we are looking for the weights w (which includes the bias term) such that it mimimizes our loss. In order to find this w, we use the gradient descent framework, which searches for this local minima. In this framework, we compute the gradient of our loss function: \\[\\ell(\\hat{y}, y)=-y\\log\\sigma(\\hat{y})-(1-y)\\log(1-\\sigma(\\hat{y})),\\] where \\(\\sigma\\) is the logistic sigmoid function and \\(\\hat{y}\\) is our prediction \\(\\langle w,x_i \\rangle\\). This loss function, known as the logistic loss function, is convient for us because it is strictly convex in it’s first argument meaning that our loss can have at most one minimum. The gradient of this loss function turns out to be: \\[\\nabla L(w)=(1/n)\\sum_{i=1}^n (\\sigma(\\hat{y_i})-y_i)x_i.\\] This gradient equation is implemented in the gradient function of the code like\nnp.mean(((self.sigmoid(y_) - y)[:,np.newaxis]) * X, axis = 0).\nThen, as stated in Theorem 2 of Optimization with Gradient Descent notes, because our gradent is a descent direction, we adjust our w by stepping in the direction of descent since we are looking for a w such that our loss is at the lowest it can be. We do this until we either reach the specified max_epochs or convergence. In this case, convergence is until the improvement in the our loss function is small enough in magnitiude.\nThe fit_stochastic function is very similar to the fit function, expect this time we don’t compute the complete gradient, we instead compute the gradient on a batch size.\n\n\n\nBefore we conduct any experiments, we need to import and define relevant extensions, classes, and functions.\n\nfrom logistic_regression import LogisticRegression # source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\ndef graph_fit_and_loss(X, y, LR):\n    fig, axarr = plt.subplots(1, 2)\n\n    axarr[0].scatter(X[:,0], X[:,1], c = y)\n    axarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.loss_history[-1]}\")\n\n    f1 = np.linspace(-3, 3, 101)\n\n    p = axarr[0].plot(f1, (- LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\n    axarr[1].plot(LR.loss_history)\n    axarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n    plt.tight_layout()\n\n\n\nWe first created a set of data with 2 features and see that their labels slightly overlaps with each other.\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can then fit this data using our fit method, which uses gradient descent, and fit_stochastic, which used batch gradient descent. When we give these fit methods a reasonable learning rate \\(\\alpha\\), then we would expect them to converge.\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 10000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\nAfter running these methods, we see that the data was able to converge in both cases. We see that they both reach the same loss, but the stochastic gradient descent reaches it in fewer iterations, meaning it iterated over all data points less times. This may be because it makes updates to w more frequently than regular gradient descent does, since it samples its w on smaller portions of data set X and updates as it goes.\n\n\n\n\nIt’s important that the learning rate \\(\\alpha\\) is relatively small. Before we set our alpha to 0.1, but if we use the same data and set it to a too high number we will see that we never converge in both regular gradient descent and stochastic gradient descent. If it’s too large, we might be updating w by too much such that it eventually skips over the minimum.\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 125, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 50, max_epochs = 1000, batch_size = 50)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\nLR.fit(X, y, alpha = 50, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\nWe see, however, that because stochastic gradient descent updates w more often depending on our batch_size, our fit_stochastic doesn’t converge with a high learning rate that our regular gradient descent method can converge with.\n\n\n\nNow, we will run some experiments on how the batch size influences convergence. We will do this with a larger data set that has 10 features.\n\n# Make the data\np_features = 11\nX, y = make_blobs(n_samples = 1000, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\ndef graph_loss(LR):\n    fig, axarr = plt.subplots(1, 1)\n\n    axarr.plot(LR.loss_history)\n    axarr.set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = f\"Loss = {LR.loss_history[-1]}\")\n    plt.tight_layout()\n\nIn the Disenroth, Faisal, and Soon reading, they observe that “Large mini-batch sizes will provide accurate estimates of the gradient, reducing the variance in the parameter update. … The reduction in variance leads to more stable convergence, but each graident calculation will be more expensive” (232) On the other hand, they observe that “small mini-batches are quick to estimate. If we keep the mini-batch size small, the noise in our graident estimate will allow us to get out of some bad local optima” (232). We’ll conduct experiments such that our batch_size decreases in size, and we’ll inspect their loss to see how Disenroth, Faisal, and Soon’s observations hold.\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 500)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 50)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 25)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 5)\n\ngraph_loss(LR)\n\n\n\n\nWe see that as our batch size gets smaller, our number of iterations over the all data points in X also gets smaller. This relates to why stochastic gradient descent uses fewer iterations as gradient descent. As our batch size gets larger, we do not updates w as frequently as we would have if it were smaller. However, stochastic gradient descent with large batch sizes"
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Source Code: CHECK LINK\n\n\nConstruct at least one interesting displayed figure (e.g. using seaborn) and at least one interesting displayed table (e.g. using pandas.groupby().aggregate). Make sure to include a helpful discussion of both the figure and the table. Don’t just show the result: explain what you learned about the data from these products.\n\n\n\nFind three features of the data and a model trained on those features which achieves 100% testing accuracy. You must obtain your three features through a reproducible process. That is, you can’t just pick them: you need to code up some kind of search in order to obtain them. - One feature must be qualitative (like Island or Clutch Completion). - The other two features must be quantitative (like Body Mass (g) or Culmen Depth (mm)).\n\n\n\nShow the decision regions of your finished model, split out by the qualitative feature."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#data-preparation",
    "href": "posts/palmer-penguins/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n# Prepare qualitative data and mark species as labels\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\n# Prepare training data\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#question",
    "href": "posts/palmer-penguins/index.html#question",
    "title": "Classifying Palmer Penguins",
    "section": "Question",
    "text": "Question\nPredict Question: Can we predict the species of a penguin given information on their bodies, etc?\nQUESTIONS ABOUT HOW THE FEATURES INFLUENCE THE LABEL\nLook at the features and ask questions about whether they influence what label they will be:\n\n# INSPECTING DATA \nprint(\"BASE RATES\") \nprint(\"y_train:\", y_train)\nprint(\"species 0 count:\", np.count_nonzero(y_train == 0), \"percentage:\", np.count_nonzero(y_train == 0)/np.size(y_train)) \nprint(\"species 1 count:\", np.count_nonzero(y_train == 1), \"percentage:\", np.count_nonzero(y_train == 1)/np.size(y_train)) \nprint(\"species 2 count:\", np.count_nonzero(y_train == 2), \"percentage:\", np.count_nonzero(y_train == 2)/np.size(y_train)) \n\n# WAIT SO IS OUR BASE RATE THE AVERAGE OF THESE? \n\nBASE RATES\ny_train: [2 0 0 1 0 0 0 2 0 2 0 0 1 1 1 2 1 2 2 0 0 1 2 2 0 2 0 1 1 0 0 0 2 0 2 0 0\n 0 2 0 0 0 0 0 1 0 2 2 2 1 1 1 2 2 2 0 2 0 2 2 2 0 2 1 0 0 2 0 2 2 0 2 0 0\n 2 1 1 2 2 0 1 2 2 2 1 0 1 0 0 0 0 1 2 0 2 0 0 2 0 2 2 0 0 1 0 2 0 2 0 2 0\n 0 2 2 0 2 0 2 0 2 0 2 0 2 2 0 2 0 2 0 2 2 0 2 2 0 1 2 1 2 0 0 0 2 0 0 1 1\n 0 2 1 2 2 2 2 0 2 0 0 0 0 1 0 0 2 2 0 2 0 1 0 1 2 1 1 1 2 0 1 0 0 1 0 0 2\n 0 1 1 0 1 1 0 0 1 0 2 1 2 0 1 2 0 2 2 1 1 0 2 0 0 2 1 0 2 2 1 1 2 2 2 0 2\n 2 2 2 0 0 1 2 1 2 2 1 2 0 1 0 0 0 0 1 2 0 1 1 0 2 0 1 1 1 0 2 0 2 2]\nspecies 0 count: 106 percentage: 0.4140625\nspecies 1 count: 55 percentage: 0.21484375\nspecies 2 count: 95 percentage: 0.37109375"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#choosing-features",
    "href": "posts/palmer-penguins/index.html#choosing-features",
    "title": "Classifying Palmer Penguins",
    "section": "Choosing Features",
    "text": "Choosing Features\nMight need combinations function from the itertools package\nUSE CROSS-VALIDATION! Simplest way to guard against overfitting issues and get a good feeling for how your model might do on unseen data.\n\n# WHAT IS THIS CODE DOING??? \n\n# GIVEN STARTER CODE\n\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    # \n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#model-choices",
    "href": "posts/palmer-penguins/index.html#model-choices",
    "title": "Classifying Palmer Penguins",
    "section": "Model Choices",
    "text": "Model Choices\nRemember: We are working with 3 label now not 2.\n\nfrom sklearn.linear_model import LogisticRegression\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columns\n# as suggested in the code block above\ncols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.6640625\n\n\nSince scikit-learn makes it so easy to experiment, this blog post is a great opportunity to explore some out-of-the-box models that we haven’t discussed in class. I’d suggest:\n\nfrom sklearn.tree import DecisionTreeClassifier. This one has a max_depth parameter that controls the complexity of the model. Use cross-validation to find a good value of the parameter.\nfrom sklearn.ensemble import RandomForestClassifier. State-of-the-art before the rise of neural networks.\nfrom sklearn.svm import SVC. Another state-of-the-art algorithm before the rise of neural networks. Has a parameter gamma that controls the complexity of the model. Again, use cross-validation to select gamma. It’s important to let gamma cover a wide range of values, e.g. gamma = 10**np.arange(-5, 5).\n\nYou can find a more thorough listing of models on this page."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#need-to-test-and-plot-decision-regions",
    "href": "posts/palmer-penguins/index.html#need-to-test-and-plot-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "NEED TO TEST AND PLOT DECISION REGIONS",
    "text": "NEED TO TEST AND PLOT DECISION REGIONS\nHas a link to use for testing later"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#download-training-data",
    "href": "posts/palmer-penguins/index.html#download-training-data",
    "title": "Classifying Palmer Penguins",
    "section": "Download training data",
    "text": "Download training data\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\n# Load training data\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#plotting-decision-regions",
    "href": "posts/palmer-penguins/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting Decision Regions",
    "text": "Plotting Decision Regions\nCode provided with …\n\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef decision_region_panel(X, y, model, qual_features):  \n  p = len(qual_features)\n  fig, axarr = plt.subplots(1, p, figsize=(4*p,4))\n  for i in range(p):\n\n      filler_feature_values = {2+j: 0 for j in range(p)}\n\n      filler_feature_values.update({2+i: 1})\n\n      ix = X[qual_features[i]] == 1\n\n      ax = axarr[i]\n\n      plot_decision_regions(np.array(X[ix]), y[ix], clf=model,\n                            filler_feature_values=filler_feature_values,\n                            filler_feature_ranges={2+j: 0.1 for j in range(p)},\n                            legend=2, ax=ax)\n\n      ax.set_xlabel(X.columns[0])\n      ax.set_ylabel(X.columns[1])\n\n      handles, labels = ax.get_legend_handles_labels()\n      ax.legend(handles, \n          [\"Adelie\", \"Chinstrap\", \"Gentoo\"], \n           framealpha=0.3, scatterpoints=1)\n\n  # Adding axes annotations\n  fig.suptitle(f'Accuracy = {model.score(X, y).round(3)}')\n  plt.tight_layout()\n  plt.show()\n\n\nqual_features = [\"Clutch Completion_No\", \"Clutch Completion_Yes\"]\ndecision_region_panel(X_train[cols], y_train, LR, qual_features)\n\n/Users/katiemacalintal/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n/Users/katiemacalintal/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn("
  }
]