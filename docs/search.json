[
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Source Code: perceptron.py\nThe perceptron algorithm is a binary classification algorithm. It works to find a hyperplane that splits the given data into their respective labels. This machine learning algorithm, however, has the limitations that it only works on linearly separable data and data separated only into two groups.\n\n\nIn our Perceptron class, we implemented a fit(X, y) algorithm, which finds the variables of weights, w, that linearly separates X, a data set of observations and their features according to their labels y, such that an element in y is either 0 or 1. In this method, we first start with a random value for w. Then while we haven’t reached a 100% classification rate and have not exceeded the maximum number of steps, we continue to tweak our w depending on if it was able to correctly classify a random point i in our data set X.\nThe update of our weights, w, is seen on line 45 of our perceptron.py code. It is equivalent to the following math equation … \\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y}_i {\\langle \\tilde{w}^{(t)}, \\tilde{x}_i\\rangle} < 0)\\tilde{y}_i \\tilde{x}_i\\]\nThis equation only updates w when our prediction for the label of \\(\\tilde{x}_i\\), the randomly selected data point, is incorrect. This is indicated through the \\(\\mathbb{1}(\\tilde{y}_i {\\langle \\tilde{w}^{(t)}, \\tilde{x}_i\\rangle} < 0)\\) portion of the equation, for it will evaulate to 0 when our w does classify the point correctly and the calcuations will not affect \\(\\tilde{w}^{(t+1)}\\), our w in the next iteration. When the indicator evaulates to 1, meaning our predication was incorrect, then our w is incremented by \\(\\tilde{y}_i \\tilde{x}_i\\). This moves our linear separator in the right direction, so that our randomly selected point could be classified correctly on the next iteration if it were selected.\n\n\n\nBefore we conduct any experiments, we need to import and define relevant extensions, classes, and functions.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron\n\nnp.random.seed(12345)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n\n\nHere is a data set of two distinct groups with 2 features.\n\nn = 100\np_features = 2\n\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(-1.75, -1.75), (1.75, 1.75)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAs shown in the graph, it looks like the points could be separated by a line so that all the purple points, whose label would be “0,” are on one side of the line and all the yellow points, whose label would be “1,” are on the other side of the line.\nWhen we run our Perceptron fit method on this data, we can inspect it’s history of scores. We see that over time the score flucuated as w was adjusted, but it eventually reaches 100% classification. This means that the fit method eventually found the variable of weights that would allow it to create a linear separator on the given data.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nWhen we then graph our line using the weights our Perceptron fit method found, we see that our algorithm was indeed able to separate the data into their respective labels.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nHere is another data set of two distinct groups with 2 features.\n\nn = 100\np_features = 2\n\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(0, 0), (0, 0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThis time, however, it does not look like we can separate the purple and yellow points from each other. Since the data points overlap, it does not look like a linear separator could separate the points into their respective labels.\nWhen looking at this set’s history of scores, we see that it too flucates. This time, however, we see that the score never reaches 1, in other words, it does not reach 100% classification. Instead, it eventually reaches the max number of times we’ll adjust w. While the Perceptron fit method still stores the last value of weights in the object, it communicates that it is not accurate as it gives the warning that the data was unable to converge.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n/Users/katiemacalintal/Desktop/Machine Learning/KatieMacalintal.github.io/posts/perceptron/perceptron.py:51: UserWarning: WARNING: Could not converge\n  warnings.warn(\"WARNING: Could not converge\")\n\n\n\n\n\nWhen we then graph our line using the weights the final iteration of our Perceptron fit method found, we see that, as expected, our algorithm was not able to separate the data into their expected labels. (Note: The scale of the axis may also look different)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nOur perceptron algorithm can also work in more than 2 dimensions. This time we’ve created a data set of two distinct groups with 7 features.\n\nn = 100\np_features = 7\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(-1, -1, -1, -1, -1, -1, -1), (1, 1, 1, 1, 1, 1, 1)])\n\nSince this is difficult to visualize, we will only be inspecting its history of scores. As we can see in the chart, our new data set is linearly separable as the score eventually reaches 1.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nWe can also alter this data set, so that the two groups overlap.\n\nn = 100\np_features = 7\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(10, 10, 10, 10, 10, 10, 10), (10, 10, 10, 10, 10, 10, 10)])\n\nAs we can see by inspecting the history of scores and the warning that is thrown, our data set never converges.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n/Users/katiemacalintal/Desktop/Machine Learning/KatieMacalintal.github.io/posts/perceptron/perceptron.py:51: UserWarning: WARNING: Could not converge\n  warnings.warn(\"WARNING: Could not converge\")\n\n\n\n\n\n\n\n\n\nIn the context of a single iteration of the perceptron algorithm update, I think the runtime complexity of our algorithm would depend on \\(p\\), the number of features. The operation that I think take the longest time would be \\({\\langle \\tilde{w}^{(t)}, \\tilde{x}_i\\rangle}\\), which I think takes \\(O(p)\\) time. There is other multiplication and addition that takes place in this equation, but since it happens consecutively it won’t matter at a larger scale. Thus, I think that runtime of the update would be \\(O(p)\\)."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Reflect on Dr. Timnit Gebru’s work and broad talk to the Middlebury campus\n\n\n\n\n\n\nApr 19, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplement and experiment with a simple ML approach for image compression\n\n\n\n\n\n\nApr 12, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nCreate a machine learning model that predicts whether an individual has an income of over $50K. Perform a fairness audit in order to assess whether or not the algorithm displays bias with respect to sex.\n\n\n\n\n\n\nMar 29, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplement least-squares linear regression and experiment with LASSO regularization for overparameterized problems\n\n\n\n\n\n\nMar 15, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nDetermine the smallest number of measurements necessary to confidently determine the species of a penguin\n\n\n\n\n\n\nMar 8, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplement first-order methods, including simple gradient descent and stochastic gradient descent with momentum, comparing their performance for training logistic regression\n\n\n\n\n\n\nMar 2, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplement and explain the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets\n\n\n\n\n\n\nFeb 22, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Katie Macalintal\nMiddlebury College\nB.A. Computer Science\n2024"
  },
  {
    "objectID": "posts/optimization/index.html",
    "href": "posts/optimization/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Source Code: logistic-regression.py\nIn this blog post, we use logistic regression and gradient descent to efficiently find a hyperplane that can separate a binary classified data set with minimal loss, in other words minimize the empircal risk.\n\n\nIn our LogisticRegression class, we implemented a fit and fit_stochastic function that both take the data set X and their expected labels y.\nIn the fit function, we are looking for the weights w (which includes the bias term) such that it mimimizes our loss. In order to find this w, we use the gradient descent framework with a convex loss function, which combined allows us to search for this local minima. In this framework, we compute the gradient of our loss function: \\[\\ell(\\hat{y}, y)=-y\\log\\sigma(\\hat{y})-(1-y)\\log(1-\\sigma(\\hat{y})),\\] where \\(\\sigma\\) is the logistic sigmoid function and \\(\\hat{y}\\) is our prediction \\(\\langle w,x_i \\rangle\\). This loss function, known as the logistic loss function, is convient for us because it is strictly convex in it’s first argument meaning that our loss can have at most one minimum. The gradient of this loss function turns out to be: \\[\\nabla L(w)=(1/n)\\sum_{i=1}^n (\\sigma(\\hat{y_i})-y_i)x_i.\\] This gradient equation is implemented in the gradient function of our LogisticRegressions class as\nnp.mean(((self.sigmoid(y_) - y)[:,np.newaxis]) * X, axis = 0).\nThen, as stated in Theorem 2 of Optimization with Gradient Descent notes, because our gradent is a descent direction, we adjust our w by stepping in the direction of descent, since we are looking for a w such that our loss is at the lowest it can be. We do this until we either reach the specified max_epochs or converge. In this case, convergence is until the improvement in the our loss function is small enough in magnitiude.\nThe fit_stochastic function is very similar to the fit function, expect here we don’t compute the complete gradient, we instead compute the gradient on a specified batch size. In this function, there is also the option to specify whether one would like to use momentum.\n\n\n\nBefore we conduct any experiments, we need to import and define relevant extensions, classes, and functions.\n\nfrom logistic_regression import LogisticRegression # source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# Graph fitted line and loss function\ndef graph_fit_and_loss(X, y, LR):\n    fig, axarr = plt.subplots(1, 2)\n\n    axarr[0].scatter(X[:,0], X[:,1], c = y)\n    axarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.loss_history[-1]}\")\n\n    f1 = np.linspace(-3, 3, 101)\n\n    p = axarr[0].plot(f1, (- LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\n    axarr[1].plot(LR.loss_history)\n    axarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n    plt.tight_layout()\n\n\n\nWe first create a set of data with 2 features and see that their labels slightly overlap with each other.\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can then fit this data using our fit method, which uses gradient descent, and fit_stochastic, which used batch gradient descent. When we give these fit methods a reasonable learning rate \\(\\alpha\\), then we would expect them to converge.\n\n# Fit the model\nLR_reg = LogisticRegression()\nLR_reg.fit(X, y, alpha = 0.1, max_epochs = 10000)\n\ngraph_fit_and_loss(X, y, LR_reg)\nplt.savefig('gradient_descent.png')\n\n\n\n\n\n# Fit the model\nLR_stoch = LogisticRegression()\nLR_stoch.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10)\n\ngraph_fit_and_loss(X, y, LR_stoch)\n\n\n\n\n\nnum_steps = len(LR_reg.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_reg.loss_history, label = \"gradient\")\n\nnum_steps = len(LR_stoch.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_stoch.loss_history, label = \"stochastic gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nAfter running these methods, we see that the data was able to converge in both cases. We see that they both reach the same loss, but the stochastic gradient descent reaches it in fewer iterations, meaning it iterated over all data points less times. This may be because it makes updates to w more frequently than regular gradient descent does, since it samples its w on smaller portions of data set X and updates it appropriately.\n\n\n\n\nIt’s important that the learning rate \\(\\alpha\\) is relatively small. Before we set our alpha to 0.1, but if we use the same data and set \\(\\alpha\\) to a too high number we will see that we never converge in both regular gradient descent and stochastic gradient descent. If \\(\\alpha\\) too large, we might be updating w by too much such that it overshoots where the minimum actually is resulting in a flucuating loss history.\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 170, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n/Users/katiemacalintal/Desktop/Machine Learning/KatieMacalintal.github.io/posts/optimization/logistic_regression.py:54: UserWarning: WARNING: Could not converge\n  warnings.warn(\"WARNING: Could not converge\")\n\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 75, max_epochs = 1000, batch_size = 50)\n\ngraph_fit_and_loss(X, y, LR)\n\n/Users/katiemacalintal/Desktop/Machine Learning/KatieMacalintal.github.io/posts/optimization/logistic_regression.py:119: UserWarning: WARNING: Could not converge\n  warnings.warn(\"WARNING: Could not converge\")\n\n\n\n\n\n\nLR.fit(X, y, alpha = 75, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\nThrough graphing the loss history, we see that it fluctuates greatly due to the large \\(\\alpha\\) size. We also see that because stochastic gradient descent updates w more often, our fit_stochastic doesn’t converge with a high learning rate that our regular gradient descent method can actually converge with.\n\n\n\nNow, we will run some experiments on how the batch size influences convergence. We will do this with a larger data set of 1000 points, which have 10 features.\n\n# Make the data\np_features = 11\nX, y = make_blobs(n_samples = 1000, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\ndef graph_loss(LR):\n    fig, axarr = plt.subplots(1, 1)\n\n    axarr.plot(LR.loss_history)\n    axarr.set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = f\"Loss = {LR.loss_history[-1]}\")\n    plt.tight_layout()\n\nIn the Disenroth, Faisal, and Soon reading, they observe that “Large mini-batch sizes will provide accurate estimates of the gradient, reducing the variance in the parameter update. … The reduction in variance leads to more stable convergence, but each graident calculation will be more expensive” (232). On the other hand, they observe that “small mini-batches are quick to estimate. If we keep the mini-batch size small, the noise in our graident estimate will allow us to get out of some bad local optima” (232). We’ll conduct experiments such that our batch_size decreases in size, and we’ll inspect their loss to see how Disenroth, Faisal, and Soon’s observations hold.\n\nLR_500 = LogisticRegression()\nLR_500.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 500)\n\ngraph_loss(LR_500)\n\n\n\n\n\nLR_50 = LogisticRegression()\nLR_50.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 50)\n\ngraph_loss(LR_50)\n\n\n\n\n\nLR_25 = LogisticRegression()\nLR_25.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 25)\n\ngraph_loss(LR_25)\n\n\n\n\n\nLR_5 = LogisticRegression()\nLR_5.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 5)\n\ngraph_loss(LR_5)\n\n\n\n\n\nnum_steps = len(LR_500.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_500.loss_history, label = \"batch size 500\")\n\nnum_steps = len(LR_50.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_50.loss_history, label = \"batch size 50\")\n\nnum_steps = len(LR_25.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_25.loss_history, label = \"batch size 25\")\n\nnum_steps = len(LR_5.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_5.loss_history, label = \"batch size 5\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nWe see that as our batch size gets smaller, our number of iterations over the all data points in X also gets smaller. This relates to why stochastic gradient descent uses fewer iterations as gradient descent. If our batch size is large, we do not update w as frequently as we would have if it were smaller. Based on our loss graphs, we also see that, as noted by Disenroth, Faisal, and Soon, stochastic gradient descent with large batch sizes have a smooth convergence, and small batch sizes have a noiser convergence.\n\n\n\nIn the fit_stochastic function we also implemented the option to use momentum, which takes into account our previously taken step and allows us to continue moving along that direction if the update was good. In the case shown below, we see that momentum was able to result in less iterations over the data, but understanding when exactly momentum would significantly speed up convergence will require more experiments.\n\n# Make the data\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (0, 0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nLR = LogisticRegression()\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = False)\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = True)\ngraph_fit_and_loss(X, y, LR)"
  },
  {
    "objectID": "posts/optimization/momentum_experiments/index.html",
    "href": "posts/optimization/momentum_experiments/index.html",
    "title": "CSCI 0451 Blog",
    "section": "",
    "text": "In this function, we also can use momentum, which takes into account our previously taken step and allows us to continue moving along that direction if the update was good."
  },
  {
    "objectID": "posts/optimization/momentum_experiments/index.html#momentum",
    "href": "posts/optimization/momentum_experiments/index.html#momentum",
    "title": "CSCI 0451 Blog",
    "section": "Momentum",
    "text": "Momentum\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nLR = LogisticRegression()\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = False)\ngraph_fit_and_loss(X, y, LR)\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = True)\ngraph_fit_and_loss(X, y, LR)\n\nDifficult to tell that momentum makes a difference here.\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (0, 0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = False)\ngraph_fit_and_loss(X, y, LR)\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = True)\ngraph_fit_and_loss(X, y, LR)"
  },
  {
    "objectID": "posts/Untitled Folder/index.html",
    "href": "posts/Untitled Folder/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Source Code: logistic-regression.py\nIn this blog post, we use logistic regression and gradient descent to efficiently find a hyperplane that can separate a binary classified data set with minimal loss, in other words minimize the empircal risk.\n\n\nIn our LogisticRegression class, we implemented a fit and fit_stochastic function that both take the data set X and their expected labels y.\nIn the fit function, we are looking for the weights w (which includes the bias term) such that it mimimizes our loss. In order to find this w, we use the gradient descent framework, which searches for this local minima. In this framework, we compute the gradient of our loss function: \\[\\ell(\\hat{y}, y)=-y\\log\\sigma(\\hat{y})-(1-y)\\log(1-\\sigma(\\hat{y})),\\] where \\(\\sigma\\) is the logistic sigmoid function and \\(\\hat{y}\\) is our prediction \\(\\langle w,x_i \\rangle\\). This loss function, known as the logistic loss function, is convient for us because it is strictly convex in it’s first argument meaning that our loss can have at most one minimum. The gradient of this loss function turns out to be: \\[\\nabla L(w)=(1/n)\\sum_{i=1}^n (\\sigma(\\hat{y_i})-y_i)x_i.\\] This gradient equation is implemented in the gradient function of the code like\nnp.mean(((self.sigmoid(y_) - y)[:,np.newaxis]) * X, axis = 0).\nThen, as stated in Theorem 2 of Optimization with Gradient Descent notes, because our gradent is a descent direction, we adjust our w by stepping in the direction of descent since we are looking for a w such that our loss is at the lowest it can be. We do this until we either reach the specified max_epochs or convergence. In this case, convergence is until the improvement in the our loss function is small enough in magnitiude.\nThe fit_stochastic function is very similar to the fit function, expect this time we don’t compute the complete gradient, we instead compute the gradient on a batch size.\n\n\n\nBefore we conduct any experiments, we need to import and define relevant extensions, classes, and functions.\n\nfrom logistic_regression import LogisticRegression # source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\ndef graph_fit_and_loss(X, y, LR):\n    fig, axarr = plt.subplots(1, 2)\n\n    axarr[0].scatter(X[:,0], X[:,1], c = y)\n    axarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.loss_history[-1]}\")\n\n    f1 = np.linspace(-3, 3, 101)\n\n    p = axarr[0].plot(f1, (- LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\n    axarr[1].plot(LR.loss_history)\n    axarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n    plt.tight_layout()\n\n\n\nWe first created a set of data with 2 features and see that their labels slightly overlaps with each other.\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can then fit this data using our fit method, which uses gradient descent, and fit_stochastic, which used batch gradient descent. When we give these fit methods a reasonable learning rate \\(\\alpha\\), then we would expect them to converge.\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 10000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\nAfter running these methods, we see that the data was able to converge in both cases. We see that they both reach the same loss, but the stochastic gradient descent reaches it in fewer iterations, meaning it iterated over all data points less times. This may be because it makes updates to w more frequently than regular gradient descent does, since it samples its w on smaller portions of data set X and updates as it goes.\n\n\n\n\nIt’s important that the learning rate \\(\\alpha\\) is relatively small. Before we set our alpha to 0.1, but if we use the same data and set it to a too high number we will see that we never converge in both regular gradient descent and stochastic gradient descent. If it’s too large, we might be updating w by too much such that it eventually skips over the minimum.\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 125, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 50, max_epochs = 1000, batch_size = 50)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\nLR.fit(X, y, alpha = 50, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\nWe see, however, that because stochastic gradient descent updates w more often depending on our batch_size, our fit_stochastic doesn’t converge with a high learning rate that our regular gradient descent method can converge with.\n\n\n\nNow, we will run some experiments on how the batch size influences convergence. We will do this with a larger data set that has 10 features.\n\n# Make the data\np_features = 11\nX, y = make_blobs(n_samples = 1000, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\ndef graph_loss(LR):\n    fig, axarr = plt.subplots(1, 1)\n\n    axarr.plot(LR.loss_history)\n    axarr.set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = f\"Loss = {LR.loss_history[-1]}\")\n    plt.tight_layout()\n\nIn the Disenroth, Faisal, and Soon reading, they observe that “Large mini-batch sizes will provide accurate estimates of the gradient, reducing the variance in the parameter update. … The reduction in variance leads to more stable convergence, but each graident calculation will be more expensive” (232) On the other hand, they observe that “small mini-batches are quick to estimate. If we keep the mini-batch size small, the noise in our graident estimate will allow us to get out of some bad local optima” (232). We’ll conduct experiments such that our batch_size decreases in size, and we’ll inspect their loss to see how Disenroth, Faisal, and Soon’s observations hold.\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 500)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 50)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 25)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 5)\n\ngraph_loss(LR)\n\n\n\n\nWe see that as our batch size gets smaller, our number of iterations over the all data points in X also gets smaller. This relates to why stochastic gradient descent uses fewer iterations as gradient descent. As our batch size gets larger, we do not updates w as frequently as we would have if it were smaller. However, stochastic gradient descent with large batch sizes"
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Chinstrap, Gentoo, and Adelie penguin clipart by Allison Horst"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#data-preparation",
    "href": "posts/palmer-penguins/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n# species = [s.split()[0] for s in le.classes_]\n\n# Prepare qualitative data and mark species as labels\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df)\n    return df, y\n\n# Prepare training data\nX_train, y_train = prepare_data(train)\n# X_train.head()"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#question",
    "href": "posts/palmer-penguins/index.html#question",
    "title": "Classifying Palmer Penguins",
    "section": "Question",
    "text": "Question\nPredict Question: Can we predict the species of a penguin given information on their bodies, etc?\nQUESTIONS ABOUT HOW THE FEATURES INFLUENCE THE LABEL\nLook at the features and ask questions about whether they influence what label they will be:\n\nimport seaborn as sns\nsns.set_theme()\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\"\n)\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y='Flipper Length (mm)', hue=\"Species\"\n)\n\n<seaborn.axisgrid.FacetGrid at 0x13779fdc0>"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#choosing-features",
    "href": "posts/palmer-penguins/index.html#choosing-features",
    "title": "Classifying Palmer Penguins",
    "section": "Choosing Features",
    "text": "Choosing Features\nMight need combinations function from the itertools package\nUSE CROSS-VALIDATION! Simplest way to guard against overfitting issues and get a good feeling for how your model might do on unseen data.\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\nall_qual_cols = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\n# Resource: https://www.datatechnotes.com/2021/02/seleckbest-feature-selection-example-in-python.html\n\n# Pick qualatative features\nX_qual = X_train[all_qual_cols]\nqual_selected = SelectKBest(f_classif, k=3).fit(X_qual, y_train)\nmask = qual_selected.get_support()\nqual_names = X_qual.columns[mask]\nselected_qual = X_train[qual_names]\nprint(qual_names)\n\n# Pick quantatative features \nX_quant = X_train[all_quant_cols]\nquant_select = SelectKBest(f_classif, k=2).fit(X_quant, y_train)\nmask = quant_select.get_support()\nquant_names = X_quant.columns[mask]\nselected_quant = X_train[quant_names]\nown_quant = np.array(['Culmen Length (mm)', 'Culmen Depth (mm)'])\nprint(quant_names)\n\nfeatures = np.concatenate((quant_names, qual_names))\nprint(features)\n\n# Combine them after inspecting \n# X_selected_features = pd.concat([selected_quant, selected_qual], axis=1)\n# X_selected_features\n\nIndex(['Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], dtype='object')\nIndex(['Culmen Length (mm)', 'Flipper Length (mm)'], dtype='object')\n['Culmen Length (mm)' 'Flipper Length (mm)' 'Island_Biscoe' 'Island_Dream'\n 'Island_Torgersen']\n\n\n\n#FIND MAX DEPTH OF TREE \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nfig, ax = plt.subplots(1)\n\nmax_score = 0\nbest_depth = 0\nfor d in range(2, 10):\n    # USING DECISION TREE CLASSIFER \n    T = DecisionTreeClassifier(max_depth = d)\n    cv_mean = cross_val_score(T, X_train[features], y_train, cv = 10).mean()\n    ax.scatter(d, cv_mean, color = \"black\")\n    if cv_mean > max_score:\n        max_score = cv_mean \n        best_depth = d\n\nlabs = ax.set(xlabel = \"Complexity (depth)\", ylabel = \"Performance (score)\")\n       \nprint(max_score, best_depth)\n\n0.9766153846153847 5\n\n\n\n\n\n\n#FIND GAMMA \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\n\nfig, ax = plt.subplots(1)\n\nmax_score = 0\nbest_gamma = 0\nfor g in range(1, 10):\n    little_g = g / 100\n    # USING DECISION TREE CLASSIFER \n    svc = SVC(gamma = little_g, kernel = \"rbf\")\n    cv_mean = cross_val_score(svc, X_train[features], y_train, cv = 10).mean()\n    ax.scatter(little_g, cv_mean, color = \"black\")\n    if cv_mean > max_score:\n        max_score = cv_mean \n        best_gamma = little_g\n\nlabs = ax.set(xlabel = \"Gamma\", ylabel = \"Performance (score)\")\n       \nprint(max_score, best_gamma)\n\n0.9572307692307692 0.03\n\n\n\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef decision_region_panel(X, y, model, qual_features):  \n    p = len(qual_features)\n    fig, axarr = plt.subplots(1, p, figsize=(4*p,4))\n    for i in range(p):\n\n        filler_feature_values = {2+j: 0 for j in range(p)}\n        \n        filler_feature_values.update({2+i: 1})\n        \n        ix = X[qual_features[i]] == 1\n\n        ax = axarr[i]\n        # print(f\"{X[ix]=}\")\n        plot_decision_regions(np.array(X[ix]), y[ix], clf=model,\n                            filler_feature_values=filler_feature_values,\n                            filler_feature_ranges={2+j: 0.1 for j in range(p)},\n                            legend=2, ax=ax)\n\n        ax.set_xlabel(X.columns[0])\n        ax.set_ylabel(X.columns[1])\n\n        handles, labels = ax.get_legend_handles_labels()\n        ax.legend(handles, \n          [\"Adelie\", \"Chinstrap\", \"Gentoo\"],\n           framealpha=0.3, scatterpoints=1)\n        \n    # Adding axes annotations\n    fig.suptitle(f'Accuracy = {model.score(X, y).round(3)}')\n    plt.tight_layout()\n    plt.show()\n\nDTC = DecisionTreeClassifier(max_depth = best_depth)\nDTC.fit(X_train[features], y_train)\n# print(DTC.score(X_train[features], y_train))\n# decision_region_panel(X_train[features], y_train, DTC, qual_names)\n\nsvc = SVC(gamma = best_gamma) \nsvc.fit(X_train[features], y_train)\n# print(svc.score(X_train[features], y_train))\n# decision_region_panel(X_train[features], y_train, svc, qual_names)\n\nLR = LogisticRegression()\nLR.fit(X_train[features], y_train)\n# print(LR.score(X_train[features], y_train))\nprint(f\"features: {features}\")\nprint(f\"qual_names: {qual_names}\")\ndecision_region_panel(X_train[features], y_train, LR, qual_names)\n\nfeatures: ['Culmen Length (mm)' 'Flipper Length (mm)' 'Island_Biscoe' 'Island_Dream'\n 'Island_Torgersen']\nqual_names: Index(['Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], dtype='object')\n\n\n\n\n\n\n# TESTING\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\nX_test, y_test = prepare_data(test)\n\n\nprint(DTC.score(X_test[features], y_test))\nprint(svc.score(X_test[features], y_test))\nprint(LR.score(X_test[features], y_test))\n# YOU GET 100% TESTING ACCURACY WITH QUANTATIVE culmen length and culmen depth \n\n\n# NOT USING \nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n\n# all_qual_cols = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\"]\nall_qual_cols = [\"Island\", \"Clutch\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\n# Create dataframe to better inspect the scores\npd.set_option('max_colwidth', 100)\nscores_df = pd.DataFrame(columns=['Columns', 'Score'])\n\n# Go through possible combinations of features and train model on them \n#     Using 1 qualitative and 2 quantiative \nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col ]\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n        # print(cols)\n        # Using logistic regression for modeling \n        LR = LogisticRegression()\n        # Incorportating cross validation? \n        cv_scores = cross_val_score(LR, X_train[cols], y_train, cv=10)\n        mean_score = cv_scores.mean()\n        scores_df = scores_df.append({'Columns': cols, 'Score': mean_score.round(3)}, ignore_index=True)\n\nscores_df = scores_df.sort_values(by='Score', ascending=False).reset_index(drop=True)\nscores_df\n    \n    #LR.fit(X_train[cols], y_train)\n    #LR.score(X_train[cols], y_train)\n    # print(f\"Features: {cols} \\n\\tscore: {mean_score.round(3)}\")\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#model-choices",
    "href": "posts/palmer-penguins/index.html#model-choices",
    "title": "Classifying Palmer Penguins",
    "section": "Model Choices",
    "text": "Model Choices\nRemember: We are working with 3 label now not 2.\n\n# from sklearn.linear_model import LogisticRegression\n\n# # this counts as 3 features because the two Clutch Completion \n# # columns are transformations of a single original measurement. \n# # you should find a way to automatically select some better columns\n# # as suggested in the code block above\n# cols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\n# LR = LogisticRegression()\n# LR.fit(X_train[cols], y_train)\n# LR.score(X_train[cols], y_train)\n\nSince scikit-learn makes it so easy to experiment, this blog post is a great opportunity to explore some out-of-the-box models that we haven’t discussed in class. I’d suggest:\n\nfrom sklearn.tree import DecisionTreeClassifier. This one has a max_depth parameter that controls the complexity of the model. Use cross-validation to find a good value of the parameter.\nfrom sklearn.ensemble import RandomForestClassifier. State-of-the-art before the rise of neural networks.\nfrom sklearn.svm import SVC. Another state-of-the-art algorithm before the rise of neural networks. Has a parameter gamma that controls the complexity of the model. Again, use cross-validation to select gamma. It’s important to let gamma cover a wide range of values, e.g. gamma = 10**np.arange(-5, 5).\n\nYou can find a more thorough listing of models on this page."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#need-to-test-and-plot-decision-regions",
    "href": "posts/palmer-penguins/index.html#need-to-test-and-plot-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "NEED TO TEST AND PLOT DECISION REGIONS",
    "text": "NEED TO TEST AND PLOT DECISION REGIONS\nHas a link to use for testing later"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#download-training-data",
    "href": "posts/palmer-penguins/index.html#download-training-data",
    "title": "Classifying Palmer Penguins",
    "section": "Download Training Data",
    "text": "Download Training Data\nFirst, we download our given training data.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load training data\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#plotting-decision-regions",
    "href": "posts/palmer-penguins/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting Decision Regions",
    "text": "Plotting Decision Regions\nCode provided with …\n\nqual_features = [\"Clutch Completion_No\", \"Clutch Completion_Yes\"]\ndecision_region_panel(X_train[cols], y_train, LR, qual_features)\n\nKeyError: 'Clutch Completion_No'"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#prepare-training-data",
    "href": "posts/palmer-penguins/index.html#prepare-training-data",
    "title": "Classifying Palmer Penguins",
    "section": "Prepare Training Data",
    "text": "Prepare Training Data\nNext, we tidy up our data. We remove any columns that are irrelevant to determining the species of a penguin and modify any qualitative features (e.g. sex, clutch completion, island), so that they are represented through numerical values instead of strings, since strings are difficult to quantify and work with.\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n\"\"\"\nPrepare qualitative data and mark species as labels\n\"\"\"\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df)\n    return df, y\n\n# Prepare training data\nX_train, y_train = prepare_data(train)"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#explore-feature-selection",
    "href": "posts/palmer-penguins/index.html#explore-feature-selection",
    "title": "Classifying Palmer Penguins",
    "section": "Explore: Feature Selection",
    "text": "Explore: Feature Selection\nNow that we have prepared our training data, we want to figure our which three features of the data (two quantitative and one qualitative) will allow a model to achieve 100% testing accuracy when trained on those features.\nThe first way in which we tried to select these features was through the SelectKBest and f_classif functions in the sklearn.feature_selection package.\n\n# Resource: https://www.datatechnotes.com/2021/02/seleckbest-feature-selection-example-in-python.html\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nall_qual_cols = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\n# Pick quantatative features \nX_quant = X_train[all_quant_cols]\nquant_select = SelectKBest(f_classif, k=2).fit(X_quant, y_train)\nmask = quant_select.get_support()\nquant_names = X_quant.columns[mask]\n\n# Pick qualatative features\nX_qual = X_train[all_qual_cols]\nqual_selected = SelectKBest(f_classif, k=3).fit(X_qual, y_train)\nmask = qual_selected.get_support()\nqual_names = X_qual.columns[mask]\n\nfeatures = np.concatenate((quant_names, qual_names))\n\n\nprint(f\"quant_names: {quant_names}\")\nprint(f\"qual_names: {qual_names}\")\nprint(f\"features: {features}\")\n\nquant_names: Index(['Culmen Length (mm)', 'Flipper Length (mm)'], dtype='object')\nqual_names: Index(['Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], dtype='object')\nfeatures: ['Culmen Length (mm)' 'Flipper Length (mm)' 'Island_Biscoe' 'Island_Dream'\n 'Island_Torgersen']\n\n\nWhen we inspect the features SelectKBest chose based on the f_classif score function, we see that it found the quantative Culmen Length (mm) and Flipper Length (mm) features and qualitative Island feature to be our most useful features with the highest scores.\nSince our data doesn’t have too many features, another way in which we could have selected features was through an exhaustive search that uses the combinations function from the itertools package. To guard ourselves from overfitting issues, we use cross validation throughout this process with LogisticRegression as our model.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nall_qual_cols = [\"Island\", \"Clutch\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\n# Create dataframe to better inspect the scores\npd.set_option('max_colwidth', 100)\nscores_df = pd.DataFrame(columns=['Columns', 'Score'])\n\n# Go through possible combinations of features and train model on them \n#     Using 1 qualitative and 2 quantiative \nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col ]\n    for pair in combinations(all_quant_cols, 2):\n        cols = list(pair) + qual_cols \n        # Using logistic regression for modeling \n        LR = LogisticRegression()\n        # Incorportating cross validation\n        cv_mean_score = cross_val_score(LR, X_train[cols], y_train, cv=10).mean()\n        scores_df = scores_df.append({'Columns': cols, 'Score': cv_mean_score.round(3)}, ignore_index=True)\n\nscores_df = scores_df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n\n\nfeatures = scores_df.iloc[0,0]\nfeatures\n\n['Culmen Length (mm)',\n 'Culmen Depth (mm)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']\n\n\nWe see that the exhaustive search also found the qualitative Island feature to be most useful. We can further inspect why this qualitative Island feature was chosen over Sex and Clutch Completion using functions like groupby and aggregate from the pandas package.\n\n\"\"\"\nResources: \nhttps://jakevdp.github.io/PythonDataScienceHandbook/03.08-aggregation-and-grouping.html\nhttps://www.geeksforgeeks.org/python-pandas-dataframe-reset_index/\nhttps://towardsdatascience.com/interesting-ways-to-select-pandas-dataframe-columns-b29b82bbfb33\nhttps://sites.ualberta.ca/~hadavand/DataAnalysis/notebooks/Reshaping_Pandas.html\n\"\"\"\n\n# Group the penguins by species and island, and count the number of occurrences\ncounts = train.groupby(['Species', 'Island']).size().reset_index(name='count')\n\n# Group the penguins by island and compute the total count for each island\nisland_totals = counts.groupby('Island')['count'].sum().reset_index(name='total')\n\n# Merge the counts and island_totals\nresults = pd.merge(counts, island_totals, on='Island')\n\n# Compute the percentage\nresults['percentage'] = results['count'] / results['total'] * 100\n\n# Edit results dataframe so that it only contains the Island, Species and percentage\nresults = results[['Island', 'Species', 'percentage']]\n\n# Arrange results to have islands as columns and species as rows\nresults = results.pivot(index='Species', columns='Island', values='percentage').round(2)\n\nprint(results)\n\nIsland                                     Biscoe  Dream  Torgersen\nSpecies                                                            \nAdelie Penguin (Pygoscelis adeliae)         25.74  42.27      100.0\nChinstrap penguin (Pygoscelis antarctica)     NaN  57.73        NaN\nGentoo penguin (Pygoscelis papua)           74.26    NaN        NaN\n\n\n\ncounts = train.groupby(['Species', 'Sex']).size().reset_index(name='count')\nsex_totals = counts.groupby('Sex')['count'].sum().reset_index(name='total')\nresults = pd.merge(counts, sex_totals, on='Sex')\nresults['percentage'] = results['count'] / results['total'] * 100\nresults = results[['Sex', 'Species', 'percentage']]\nresults = results.pivot(index='Species', columns='Sex', values='percentage').round(2)\nresults = results.drop(columns='.')\n\nprint(results)\n\nSex                                        FEMALE   MALE\nSpecies                                                 \nAdelie Penguin (Pygoscelis adeliae)         44.53  40.44\nChinstrap penguin (Pygoscelis antarctica)   22.66  19.85\nGentoo penguin (Pygoscelis papua)           32.81  39.71\n\n\n\ncounts = train.groupby(['Species', 'Clutch Completion']).size().reset_index(name='count')\nclutch_totals = counts.groupby('Clutch Completion')['count'].sum().reset_index(name='total')\nresults = pd.merge(counts, clutch_totals, on='Clutch Completion')\nresults['percentage'] = results['count'] / results['total'] * 100\nresults = results[['Clutch Completion', 'Species', 'percentage']]\nresults = results.pivot(index='Species', columns='Clutch Completion', values='percentage').round(2)\n\nprint(results)\n\nClutch Completion                             No    Yes\nSpecies                                                \nAdelie Penguin (Pygoscelis adeliae)        37.93  43.50\nChinstrap penguin (Pygoscelis antarctica)  37.93  18.29\nGentoo penguin (Pygoscelis papua)          24.14  38.21\n\n\nWhen we inspect the qualitative features in this way, we see that each island has at most two different penguin species that live there. For the Sex and Clutch Completion features, however, we see that it’s harder to differentiate the species based on those values for it could be any of the three species.\nLooking back at our exhaustive search feature results, we also see, however, that it found a different pair of quantatative features with a higher sore. It found Culmen Depth (mm) and Culmen Length to be more useful features.\nTo figure out whether the SelectKBest Flipper Length (mm) and Culmen Length features or exhaustive search features Culmen Depth (mm) and Culmen Length (mm) are the better pair of quantitative features, let’s inspect what they look like when graphed using the seaborn package.\n\nimport seaborn as sns\nsns.set_theme()\n\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y='Flipper Length (mm)', hue=\"Species\"\n).set(title = \"SelectKBest Features\")\n\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\"\n).set(title = \"Exhaustive Search Features\")\n\n<seaborn.axisgrid.FacetGrid at 0x157babbb0>\n\n\n\n\n\n\n\n\nBased on these graphs, it looks like Culmen Depth (mm) and Culmen Length (mm) may be the better quantative options for it looks like they have less overlap among their species. In other words, it’s more easily separable and distinguishable.\nTo summarize when training our models, we will use the quantitative Culmen Depth and Culmen Length features and qualitative Island feature."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#explore-modeling",
    "href": "posts/palmer-penguins/index.html#explore-modeling",
    "title": "Classifying Palmer Penguins",
    "section": "Explore: Modeling",
    "text": "Explore: Modeling\nNow that we have chosen our features, we can begin to train different models using our training data. In this blog post, we explore the models of DecisionTreeClassifier, RandomForestClassifier, and LogisticRegression and use the plot_regions method below to visualize our decision regions.\n\nfrom matplotlib.patches import Patch\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n\n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n        XY = pd.DataFrame({\n            X.columns[0] : XX,\n            X.columns[1] : YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n      \n        # use contour plot to visualize the predictions\n        axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n\n        ix = X[qual_features[i]] == 1\n        # plot the data\n        axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n\n        axarr[i].set(xlabel = X.columns[0], \n                     ylabel  = X.columns[1])\n        axarr[i].set_title(qual_features[i])\n\n        patches = []\n        for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n            patches.append(Patch(color = color, label = spec))\n        \n        plt.suptitle(f\"Score {model.score(X, y)}\")\n        plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n        plt.tight_layout()\n\n\nDecisionTreeClassifier\nFor the DecisionTreeClassifer model, we need to provide it with a max_depth argument, which helps control the complexity of the model. In order to find a good max_depth value, we use cross validation to help prevent overfitting.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nfig, ax = plt.subplots(1)\n\nmax_score = 0\nbest_depth = 0\nfor d in range(2, 10):\n    T = DecisionTreeClassifier(max_depth = d)\n    cv_mean = cross_val_score(T, X_train[features], y_train, cv = 10).mean()\n    ax.scatter(d, cv_mean, color = \"black\")\n    if cv_mean > max_score:\n        max_score = cv_mean \n        best_depth = d\n\nlabs = ax.set(xlabel = \"Complexity (depth)\", ylabel = \"Performance (score)\")\n\n\n\n\nNow that we have found the most suitable max_depth, we can train our model with it and look at its decision regions.\n\nDTC = DecisionTreeClassifier(max_depth = best_depth)\nDTC.fit(X_train[features], y_train)\nplot_regions(DTC, X_train[features], y_train)\n\n\n\n\nBased on these plotted decision regions and the score, it looks like our DecisionTreeClassifier model did a good job with correctly classifying our training data. It also looks like it was able to do it without overfitting, for the graphs do not look too wiggly or tailored so much to our training data.\n\n\nRandomForestClassifier\nFor the RandomForestClassifier model, no arguments are required. So we can just train our model with our selected features.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier()\nRFC.fit(X_train[features], y_train)\nplot_regions(RFC, X_train[features], y_train)\n\n\n\n\nAgain, based on these plotted decision regions and the score, it looks like our model did a good job with correctly classifying our training data. It does, however, look like it may be overfitting the data for some of the decision regions look a bit wiggly and tailored too much towards our training data.\n\n\nLogisticRegression\nFor the LogisticRegression model, no arguments are also required. So we can just train our model with our selected features.\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\nLR.fit(X_train[features], y_train)\nplot_regions(LR, X_train[features], y_train)\n\n\n\n\nAgain, based on these plotted decision regions and the score, it looks like our model did a great job with correctly classifying our training data, even better than the DecisionTreeClassifier model. It also looks like it was able to do it without overfitting, for the graphs do not look too wiggly or tailored so much to our training data."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#testing",
    "href": "posts/palmer-penguins/index.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "Testing",
    "text": "Testing\nNow, that we have trained one model using DecisionTreeClassifer and the other using LogisticRegression, we can see which one will yield us our desired results of 100% testing accurac\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\nX_test, y_test = prepare_data(test)\n\n\nprint(DTC.score(X_test[features], y_test))\nprint(LR.score(X_test[features], y_test))\n# YOU GET 100% TESTING ACCURACY WITH QUANTATIVE culmen length and culmen depth \n# Show graphs\n\n0.9852941176470589\n1.0"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#results",
    "href": "posts/palmer-penguins/index.html#results",
    "title": "Classifying Palmer Penguins",
    "section": "Results",
    "text": "Results\n\n#FIND GAMMA \n# from sklearn.model_selection import cross_val_score\n# from sklearn.svm import SVC\n\n# fig, ax = plt.subplots(1)\n\n# max_score = 0\n# best_gamma = 0\n# for g in range(1, 100000):\n#     little_g = g / 100000\n#     # USING DECISION TREE CLASSIFER \n#     svc = SVC(gamma = little_g, kernel = \"rbf\")\n#     cv_mean = cross_val_score(svc, X_train[features], y_train, cv = 10).mean()\n#     ax.scatter(little_g, cv_mean, color = \"black\")\n#     if cv_mean > max_score:\n#         max_score = cv_mean \n#         best_gamma = little_g\n\n# labs = ax.set(xlabel = \"Gamma\", ylabel = \"Performance (score)\")\n       \n# print(max_score, best_gamma)\n\n# svc = SVC(gamma = best_gamma) \n# svc.fit(X_train[features], y_train)\n# # print(svc.score(X_train[features], y_train))\n# # decision_region_panel(X_train[features], y_train, svc, qual_names)\n# plot_regions(svc, X_train[features], y_train)\n\nKeyboardInterrupt: \n\n\nError in callback <function flush_figures at 0x14cb143a0> (for post_execute):\n\n\nKeyboardInterrupt:"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#testing-and-results",
    "href": "posts/palmer-penguins/index.html#testing-and-results",
    "title": "Classifying Palmer Penguins",
    "section": "Testing and Results",
    "text": "Testing and Results\nNow, that we have models trained using DecisionTreeClassifer, RandomForestClassifier, and LogisticRegression, we can see which one will yield us our desired results of 100% testing accuracy.\nFirst, we download and prepare our testing data.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\nX_test, y_test = prepare_data(test)\n\nNext, we can inspect the performance of each model on the testing data by plotting their decision regions.\n\nplot_regions(DTC, X_test[features], y_test)\n\n\n\n\n\nplot_regions(RFC, X_test[features], y_test)\n\n\n\n\n\nplot_regions(LR, X_test[features], y_test)\n\n\n\n\nAs we can see based off our decision regions and scores, while our DecisionTreeClassifer and RandomForestClassifier model yields promising classification, our LogisticRegression model trained on the features Culmen Length, Culmen Depth, and Islands does even better as it results in 100% testing accuracy."
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "Source Code: linear_regression.py"
  },
  {
    "objectID": "posts/linear-regression/index.html#experiments",
    "href": "posts/linear-regression/index.html#experiments",
    "title": "Implementing Linear Regression",
    "section": "Experiments",
    "text": "Experiments\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR = LinearRegression()\n    LR.fit_analytic(X_train, y_train)\n    training_scores.append(LR.score(pad(X_train), y_train))\n    validation_scores.append(LR.score(pad(X_val), y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.yscale('log')\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")"
  },
  {
    "objectID": "posts/linear-regression/index.html#lasso",
    "href": "posts/linear-regression/index.html#lasso",
    "title": "Implementing Linear Regression",
    "section": "LASSO",
    "text": "LASSO\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nL.score(X_val, y_val)\n\n0.7548622009585426"
  },
  {
    "objectID": "posts/linear-regression/index.html#lasso-1",
    "href": "posts/linear-regression/index.html#lasso-1",
    "title": "Implementing Linear Regression",
    "section": "LASSO",
    "text": "LASSO\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    L = Lasso(alpha = 0.0009)\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.yscale('log')\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\n\n\n\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    L = Lasso(alpha = 0.001)\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.yscale('log')\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\n\n\n\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    L = Lasso(alpha = 0.01)\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.yscale('log')\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\n\n\n\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    L = Lasso(alpha = 0.1)\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.yscale('log')\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")"
  },
  {
    "objectID": "posts/linear-regression/index.html#implementation",
    "href": "posts/linear-regression/index.html#implementation",
    "title": "Implementing Linear Regression",
    "section": "Implementation",
    "text": "Implementation\nResource: Regression Notes\nAs discussed in this course, least-squares linear regression fits nicely into the friendly convex linear model framework. In our LinearRegression class, we implemented a linear predict function and a loss function that uses squared error, which is convex. By defining our predict and loss functions in these ways, we are then left with the empirical risk minimization problem of \\[\\hat{w}=\\underset{w}{\\arg\\min}\\sum_{i=1}^n(\\langle{\\mathbf{w}}, {\\mathbf{x}}_i\\rangle - y_i)^{2}=\\underset{w}{\\arg\\min}\\left\\lVert\\mathbf{Xw}-\\mathbf{y}\\right\\rVert_2^2.\\]\nTo solve this empirical risk minimization problem, we first take gradient with respect to \\(\\hat{w}\\), which results in \\[\\nabla L(w)=\\mathbf{X}^T(\\mathbf{Xw}-\\mathbf{y}).\\]\nTo continue solving this empirical risk minimization problem and find \\(\\hat{w}\\), we implemented two different fit methods in our LinearRegression class: fit_analytic and fit_gradient.\nIn fit_analytic, we use a formula involving matrix inversion that is obtained by using the condition \\(\\nabla L(w)=0\\). This ultimately results in \\[\\hat{w}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}\\] and the corresponding code np.linalg.inv(X_.T@X_)@X_.T@y for w.\nIn fit_gradient, we use gradient descent and update w until the change in loss is at its lowest. To compute our gradient descent, which is \\[\\nabla L(w)=\\mathbf{X}^T(\\mathbf{Xw}-\\mathbf{y}),\\] is a very expensive computation because \\(\\mathbf{X}^T\\mathbf{X}\\) has time complexity \\(O(np^2)\\) and \\(\\mathbf{X}^T\\mathbf{y}\\) has time complexity \\(O(np)\\). But since the gradient doesn’t depend on our current w we precompute \\(\\mathbf{X}^T\\mathbf{X}\\) as \\({P}\\) and \\(\\mathbf{X}^T\\mathbf{y}\\) as \\({q}\\). This allows our w update to only take \\(O(p^2)\\) time."
  },
  {
    "objectID": "posts/linear-regression/index.html#demo",
    "href": "posts/linear-regression/index.html#demo",
    "title": "Implementing Linear Regression",
    "section": "Demo",
    "text": "Demo\nTo demonstrate that we’ve implemented the fit_analytic and fit_gradient functions of our LinearRegression class correctly, we can test it on a dataset that has 1 feature to easily visualize the problem and results. We first define relavant functions that will help us with this demo.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nNext, we create our training and validation data, each of which have 100 data points and 1 feature.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nNow we can start fitting our model using our implemented LinearRegression class. We first test out our fit_analytic function.\n\nfrom linear_regression import LinearRegression\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train)\n\nprint(f\"Training score = {LR.score(pad(X_train), y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(pad(X_val), y_val).round(4)}\")\n\nTraining score = 0.1464\nValidation score = 0.1548\n\n\nWhen we inspect our training and validation scores, we see that they may not be as high as the scores we would get on classification models in the past. This is because it all depends on the randomly generated data we get and how much noise is present in it. We can also inspect its estimated weight vector as shown below.\n\nLR.w\n\narray([0.31040324, 1.0968243 ])\n\n\nNext, we try testing out our fit_gradient function, and inspect its weight.\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, alpha = 0.01, max_iter = 1e3)\n\n\nLR2.w\n\narray([0.31041385, 1.09681821])\n\n\nWe see that its weight is pretty close to the weight calculated in our fit_analytic function. Since our fit_analytic keeps track of the scores of the current weight, we can also inspect its score_history.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nAs expected, we see the score increases monotonically in each iteration.\nBased on this demo, it looks like our LinearRegression class is working adequately."
  },
  {
    "objectID": "posts/linear-regression/index.html#linear-regression-experiments",
    "href": "posts/linear-regression/index.html#linear-regression-experiments",
    "title": "Implementing Linear Regression",
    "section": "Linear Regression Experiments",
    "text": "Linear Regression Experiments\nNext, we want to explore what overparameterization can do to our model. To do so, we run an experiment where we increase p_features, the number of features used, but keep n_train, the number of training points, constant. We do this until p_features is all the way to n_train - 1 and inspect their change in training and validation scores.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR = LinearRegression()\n    LR.fit_analytic(X_train, y_train)\n    training_scores.append(LR.score(pad(X_train), y_train))\n    validation_scores.append(LR.score(pad(X_val), y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\n\n\n\n\nWhen we inspect our evolution of training scores, we see that generally as our number of features increases, our training score also increases and eventually reaches a perfect training score. When we inspect our evolution of validation scores, however, we see that as our number of features increases, our validation score decreases and eventually shoots down to a very low validation score.\nThis evolution of the training and validation scores follows the pattern of overfitting. Here, as the number of features increase, we see a gap between the training and validation score get larger. Perhaps this tells us that the more features we use with least squares linear regression the more vulnerable we are to overfitting."
  },
  {
    "objectID": "posts/linear-regression/index.html#lasso-regularization-experiments",
    "href": "posts/linear-regression/index.html#lasso-regularization-experiments",
    "title": "Implementing Linear Regression",
    "section": "LASSO Regularization Experiments",
    "text": "LASSO Regularization Experiments\nLet’s try the same experiment with the LASSO algorithm as provided through the sklearn.linear_model. This algorithm has a modified loss function with a regularization term, which makes the entries of the weight vector \\(\\mathbf{w}\\) small. LASSO tends to force entries of the weight vector to be exactly zero, which may be desirable in our experiments where we are increasing the number of features to the number of data points.\nWe first experiment with our regularization strength set to \\({\\alpha} = 0.01\\).\n\nfrom sklearn.linear_model import Lasso\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    L = Lasso(alpha = 0.01)\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\n\n\n\n\nHere we see that as our number of features increased, our training score still increased to a close to perfect score, but our validation score is not as low as it was in least squares linear regression and the gap between these two scores is no longer as wide.\nNow let’s strengthen our regularization to \\({\\alpha} = 0.1\\).\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    L = Lasso(alpha = 0.1)\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\n\n\n\n\nHere, when our regularization is set to an even higher value, we see the gap between our training and validation score get even smaller as our number of features increase.\nLet’s now see how the LASSO algorithm with a high regularization term can handle when our number of features goes past the number of data points we have.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train + 100):\n    L = Lasso(alpha = 0.1)\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\n\n\n\n\nBased on our graph of scores, we see that the validation score fluctuates a little, but generally stays within the same range even as our number of features exceed our number of data points.\nAs we see through these experiments, the LASSO algorithm, when provided with a strong regularization term, can handle overparameterization better than our least square linear regression model."
  },
  {
    "objectID": "posts/auditing-bias/index.html",
    "href": "posts/auditing-bias/index.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "In this blog post, we are going to train a machine learning model to predict whether an individual in the state of New Jersey (NJ) has an income of over $50K based on demographic characteristics, excluding sex. After training this model, we will perform a fairness audit to assess whether or not our model displays bias with respect to sex.\n\n\nWe first download the PUMS data for the state of New Jersey and add a column to indicate whether the individual makes more than $50K. This new column is titled SAL50K and has the value 1 if the individual makes $50K or more and 0 otherwise.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"NJ\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\n# Download PUMS data for NEW JERSEY \nacs_data = data_source.get_data(states=[STATE], download=True)\n\n# Add column to indicate whether individual makes more than 50K \nacs_data = acs_data.assign(SAL50K = 1 * (acs_data['PINCP'] >= 50000))\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n      SAL50K\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000003\n      2\n      1\n      2302\n      1\n      34\n      1013097\n      12\n      23\n      ...\n      3\n      12\n      2\n      3\n      14\n      12\n      13\n      12\n      2\n      0\n    \n    \n      1\n      P\n      2018GQ0000017\n      2\n      1\n      1800\n      1\n      34\n      1013097\n      11\n      51\n      ...\n      2\n      9\n      9\n      19\n      10\n      19\n      19\n      2\n      10\n      0\n    \n    \n      2\n      P\n      2018GQ0000122\n      2\n      1\n      2104\n      1\n      34\n      1013097\n      24\n      69\n      ...\n      22\n      45\n      2\n      3\n      24\n      24\n      23\n      44\n      1\n      0\n    \n    \n      3\n      P\n      2018GQ0000131\n      2\n      1\n      800\n      1\n      34\n      1013097\n      90\n      18\n      ...\n      96\n      158\n      96\n      14\n      145\n      157\n      15\n      88\n      156\n      0\n    \n    \n      4\n      P\n      2018GQ0000134\n      2\n      1\n      1700\n      1\n      34\n      1013097\n      68\n      89\n      ...\n      67\n      68\n      69\n      6\n      123\n      131\n      132\n      127\n      68\n      0\n    \n  \n\n5 rows × 287 columns\n\n\n\nBased on the dimensions of our downloaded PUMS data, we see that there are over 250 features documented. These are too many features to train on, so in our modeling tasks, we will only focus on …\n\nAGEP is age\n\nSCHL is educational attainment\n\nMAR is marital status\n\nRELP is relationship\n\nCIT is citizenship status.\nMIG is mobility status\nMIL is miliary service\nANC is ancestry recode\nRAC1P is race (1 for White Alone, 2 for Black/African American alone, 3 for Native American alone, 4 for Alaska Native alone, 5 for Native American and Alaska Native tribes specified, 6 for Asian alone, 7 for Native Hawaiian and Other Pacific Islander alone, 8 for Some Other Race alone, 9 for Two or more races)\n\nESR is employment status (1 if employed, 0 if not)\n\nDIS, DEAR, DEYE, and DREM relate to certain disability statuses\n\nNote that we do not include …\n\nSEX is binary sex (1 for male, 2 for female)\n\nSAL50K is added column indicating whether the individual makes more than 50K\n\nfor SEX is our group choice that we will evaluate bias against and SAL50K is our target variable.\n\n# Possible features suggested to use\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR', 'SAL50K']\n\n# Subset features want to use\nfeatures_to_use = [f for f in possible_features if f not in [\"SEX\", \"PINCP\", \"SAL50K\"]]\n\nacs_data[features_to_use].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      23\n      21.0\n      5\n      17\n      2\n      NaN\n      5\n      2.0\n      4.0\n      1\n      2\n      2\n      2\n      2.0\n      6\n      6.0\n    \n    \n      1\n      51\n      20.0\n      4\n      17\n      2\n      NaN\n      4\n      1.0\n      4.0\n      1\n      2\n      2\n      2\n      2.0\n      1\n      1.0\n    \n    \n      2\n      69\n      19.0\n      3\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      4\n      1\n      2\n      2\n      2.0\n      1\n      6.0\n    \n    \n      3\n      18\n      16.0\n      5\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      1.0\n      9\n      6.0\n    \n    \n      4\n      89\n      19.0\n      2\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      4\n      1\n      2\n      2\n      1.0\n      1\n      6.0\n    \n  \n\n\n\n\nNow that we have chosen our features to use, we construct a BasicProblem that allows us to use these features to predict whether an individual makes more than $50K using the SEX as the group label. Doing this gives us our feature matrix features, our label vector label, and group label vector group.\n\nPaymentProblem = BasicProblem(\n    features=features_to_use,\n    target='SAL50K',\n    target_transform=lambda x: x == 1,\n    group='SEX',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = PaymentProblem.df_to_numpy(acs_data)\n\nFinally, we perform a train-test split on the data.\n\n# Train/test split \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\n\nBefore we train our model, we are going to inspect any patterns we see in our training data.\n\nimport pandas as pd\n\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"SEX\"] = group_train\ndf[\">50K\"] = y_train\n\ndf.shape\n\n(70868, 18)\n\n\nBased on the dimensions of our df data frame, which holds all the feature values for every individual in our training data, we see that our training data has information for about 70868 different individuals.\nBased on the table below, we see that only about 31.21% of the individuals in our training data make more than $50K.\n\nmore_than_50K = df.groupby([\">50K\"]).size().reset_index(name='count')\n\nmore_than_50K['percent'] = (more_than_50K['count'] / len(df.index) * 100).round(2)\n\nmore_than_50K\n\n\n\n\nIncome Greater Than $50K\n  \n    \n      \n      >50K\n      count\n      percent\n    \n  \n  \n    \n      0\n      False\n      48752\n      68.79\n    \n    \n      1\n      True\n      22116\n      31.21\n    \n  \n\n\n\n\nBased on the table below, we see that of those who make more than $50K in our training data, about 59% of them are male while the remaining 41% of them are female.\n\nfiltered_df = df[df['>50K'] == True]\n\nmore_than_50K_by_sex = filtered_df.groupby(['SEX']).size().reset_index(name='count')\n\nmore_than_50K_by_sex['percent'] = (more_than_50K_by_sex['count'] / len(filtered_df.index) * 100).round(2)\n\nmore_than_50K_by_sex\n\n\n\n\nSex of Those with Income Greater Than $50K\n  \n    \n      \n      SEX\n      count\n      percent\n    \n  \n  \n    \n      0\n      1\n      13035\n      58.94\n    \n    \n      1\n      2\n      9081\n      41.06\n    \n  \n\n\n\n\nBased on the two tables below, we see that about 38% of males and 25% of females have an income of more than $50K.\n\nfiltered_df = df[df['SEX'] == 1]\n\nmales = filtered_df.groupby(['>50K']).size().reset_index(name='count')\n\nmales['percent'] = (males['count'] / len(filtered_df.index) * 100).round(2)\n\nmales\n\n\n\n\nMales with Income Greater Than $50K\n  \n    \n      \n      >50K\n      count\n      percent\n    \n  \n  \n    \n      0\n      False\n      21342\n      62.08\n    \n    \n      1\n      True\n      13035\n      37.92\n    \n  \n\n\n\n\n\nfiltered_df = df[df['SEX'] == 2]\n\nfemales = filtered_df.groupby(['>50K']).size().reset_index(name='count')\n\nfemales['percent'] = (females['count'] / len(filtered_df.index) * 100).round(2)\n\nfemales\n\n\n\n\nFemales with Income Greater Than $50K\n  \n    \n      \n      >50K\n      count\n      percent\n    \n  \n  \n    \n      0\n      False\n      27410\n      75.11\n    \n    \n      1\n      True\n      9081\n      24.89\n    \n  \n\n\n\n\nOverall, based on these tables it looks like males are more likely to make more than $50K.\nWe can also inspect intersectional trends by studying the proportion of those who make more than $50K broken out by both our chosen group labels of SEX and an additional group label. Here we have chosen to use RAC1P.\nWe first create a data frame that holds that information and then use seaborn package to visualize the new data frame.\nRemember that …\n\nRAC1P is race (1 for White Alone, 2 for Black/African American alone, 3 for Native American alone, 4 for Alaska Native alone, 5 for Native American and Alaska Native tribes specified, 6 for Asian alone, 7 for Native Hawaiian and Other Pacific Islander alone, 8 for Some Other Race alone, 9 for Two or more races)\n\nSEX is binary sex (1 for male, 2 for female)\n\n\nimport seaborn as sns\n\n# Create new dataframe\nfiltered_df = df[df['>50K'] == 1]\nintersectionality = filtered_df.groupby(['SEX', 'RAC1P']).size().reset_index(name='count')\nintersectionality['Percent'] = (intersectionality['count'] / len(filtered_df.index) * 100).round(2)\n\n# Visualize it using seaborn package\nsns.set_style(\"whitegrid\")\nsns.barplot(x=\"RAC1P\", y=\"Percent\", hue=\"SEX\", data=intersectionality).set(title='Percent with Income >$50 by SEX and RAC1P')\n\n[Text(0.5, 1.0, 'Percent with Income >$50 by SEX and RAC1P')]\n\n\n\n\n\nBased on the bar plot above, we see that white men make up the majority of those who make more than $50K in our data, followed by white women. In our training data, we also see that individuals who are not white, regardless of their sex, make up a small portion of those who make more than $50K.\n\n\n\nNow it is time to train our model. The model we chose to use is the DecisionTreeClassifier, which requires a specified max_depth. To find our optimum max_depth we use cross validation.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(1)\n\nmax_score = 0\nbest_depth = 0\nfor d in range(2, 20):\n    T = DecisionTreeClassifier(max_depth = d)\n    cv_mean = cross_val_score(T, X_train, y_train, cv = 10).mean()\n    ax.scatter(d, cv_mean, color = \"black\")\n    if cv_mean > max_score:\n        max_score = cv_mean \n        best_depth = d\n\nlabs = ax.set(xlabel = \"Complexity (depth)\", ylabel = \"Performance (score)\")\n\n\n\n\nFor our model, we create a pipeline with StandardScaler() and our DecisionTreeClassifier with the our found optimum max_depth.\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nDTC = DecisionTreeClassifier(max_depth = best_depth)\nmodel = make_pipeline(StandardScaler(), DTC)\n\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=9))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=9))])StandardScalerStandardScaler()DecisionTreeClassifierDecisionTreeClassifier(max_depth=9)\n\n\n\n\n\nNow that our model is trained, we can audit how it does on the test data. We first define a function that may help with finding the positive predictive values (PPV), which is the proportion of positive results that are truly positive, the false negative rate (FNR), and false positive rate (FPR). We also extract our predictions y_hat on the test data.\n\nfrom sklearn.metrics import confusion_matrix\n\ndef find_PPV_FNR_FPR(data, predictions):\n    confusion = confusion_matrix(data, predictions)\n    TP = confusion[1][1]\n    TN = confusion[0][0]\n    FP = confusion[0][1]\n    FN = confusion[1][0]\n    PPV = TP / (TP + FP)\n    print(f\"{PPV=}\")\n    FNR = FN / (FN + TP)\n    print(f\"{FNR=}\")\n    FPR = FP / (FP + TN)\n    print(f\"{FPR=}\")\n\n# Extract predictions \ny_hat = model.predict(X_test)\n\n\n\nNow we can examine the overall accuracy, PPV, FNR, and FPR from our model on our test data.\n\n# Overall accuracy \n(y_hat == y_test).mean()\n\n0.8211987809007789\n\n\n\nfind_PPV_FNR_FPR(y_test, y_hat)\n\nPPV=0.7009745255599248\nFNR=0.2571117956151477\nFPR=0.14337240757439135\n\n\n\n\n\nWe can also examine the accuracy, PPV, FNR, and FPR for each subgroup in our test data. So examining group 1, is equivalent to examining these values for the males in the test data. And exmaining group 2, is equivalent to examining these values for the females in the test data.\n\n# Accuracy for men\n(y_hat == y_test)[group_test == 1].mean()\n\n0.8259855376720318\n\n\n\n# Accuracy for women \n(y_hat == y_test)[group_test == 2].mean()\n\n0.8167104111986002\n\n\n\n#PPV FNR and FPR for male\nfind_PPV_FNR_FPR(y_test[group_test == 1], y_hat[group_test == 1])\n\nPPV=0.7850098619329389\nFNR=0.2597644141351519\nFPR=0.12228870605833957\n\n\n\n#PPV FNR and FPR for female\nfind_PPV_FNR_FPR(y_test[group_test == 2], y_hat[group_test == 2])\n\nPPV=0.6099038118988244\nFNR=0.2533798517226341\nFPR=0.15983068165231354\n\n\nWe see that the accuracy of each group seem to be within a reasonable range of each other, but the PPV, FNR, and FPR differ slightly.\n\n\n\n\nBased on our work above and using the definition that calibration is where the PPV is the same for all groups in the data, it does not look like our model is calibrated. We see the males had a higher proportion of positively classified cases that were truly positive than the females.\nBased on the definition that error rate balance is where the FPR and FNR are the same for all groups, we see that our model does not have error rate balance. We see that FNR (predicting a negative label when it was actually positive) for males are slightly higher than the FNR for females. We also see that the FPR (predicting a positive label when it was actually negative) for males are lower than the FPR for females. Our model does not make the same mistakes for each group at the same rate. This is very interesting results based our basic descriptive of our training data where we found that females were a smaller proportion of those who made more than $50K.\nBased on the definition that statistical parity is where the PPVs are similar, we also see that our model does not satisfy statistical parity.\n\n\n\nUltimately, we feel that our model would not be fit to be deployed in the real world.\nA model like this, that predicts income labels, might be beneficial to companies that try to function in conditions where income is not explicitly disclosed, but is useful. Companies might use a model like this to be more strategic with who to advertise towards. If the brand is expensive, they might aim to advertise more towards those who have a higher income. A model like this may also be used in more serious conditions that can greatly affect someone’s way of living. Government programs that aim to assist lower income individuals may use this model to predict whether who is most in need of their help. It might also help determine who qualifies for loans, insurance, or credit cards.\nBased on our bias audit, our model seems to display all types of problematic bias. Because our model is not callibrated, the rates at which positively classified cases that are truly positive are not the same for each sex. More specifically, we see the males had a higher proportion of positively classified cases that were truly positive than the females. Because our model does not have error rate balance, it does not make the same mistakes for each group at the same rate. More specifically, we see that the rate for predicting a negative label when it was actually positive for males are slightly higher than the rate for females. We also see that the rate for predicting a positive label when it was actually negative for males are lower than the rate for females.\nDue to the results of our bias audit, our model could negatively impact males and females depending on the context in which it may be applied. For government programs that look to assist lower income individuals, this model may incorrectly predict that females make more than they do, denying them help they might really need. For loans, insurance, or credit cards, the model might predict that males make lower than they actually do, denying them these services that can make a difference in their life. We would not be comfortable releasing these models until the results of the bias audit are improved.\nBeyond bias, I think that this model may be making decisions about an individual without taking their full picture into account. When a model like this has to make big decisions related to loans, credit cards, insurance or government assistance, I think that there should be another person overlooking the models decisions to help regulate and assess the bias in the model. I also think that the individuals this model may be evaluating should have a say before a decision is made on their behalf. For example, if an individual is applying to a government program for assistance, they should be able to talk with those who make decisions to share more information about their story and better inform why they may need assistance. Ultimately, I think that it’s still important to incorporate people into these decision making processes."
  },
  {
    "objectID": "posts/unsupervised-learning/index.html",
    "href": "posts/unsupervised-learning/index.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "In this blog post, we use a linear algebra method for unsupervised learning with images. We will implement and experiment with compressing an image using singular value decomposition (SVD). To make this simpler for us, the image we will be compressing will be in greyscale.\n\n\nIn order to compress an image using singular value decomposition (SVD), we first need to select one!\n\n\n\nPhineas and Ferb\n\n\nBelow are functions that will allow us to save this image for our usage and convert it to greyscale.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\nAs seen through the image comparison below, our functions read_image and to_greyscale work as anticipated.\n\nimport pathlib\n\nurl = \"https://i.kym-cdn.com/photos/images/facebook/000/944/663/6a7.png\"\n\nimg = read_image(url)\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\n\n\n\n\n\nRecall that the singular value decomposition (SVD) of a matrix \\(\\textbf{A} \\in \\mathbb{R} ^{m x n}\\) is\n\\[\\textbf{A} = \\textbf{U} \\textbf{D} \\textbf{V}^T,\\]\nwhere \\(\\textbf{U} \\in \\mathbb{R} ^{m x m}\\) is an orthogonal matrix, \\(\\textbf{D} \\in \\mathbb{R} ^{m x n}\\) has nonzero entries (the singular values \\(\\sigma_i\\)) only along its diagonal, and \\(\\textbf{V} \\in \\mathbb{R} ^{n x n}\\) is an orthogonal matrix.\nRemember that images in greyscale are essentially matrices of n by m dimensions, where the value at each position represents the darkness of a shade of gray as a number between 0 and 255. Thus, an uncompressed greyscale image would take up \\(n*m*8\\) bits of data.\nIn the context of images, SVDs can allow us to compress a greyscale image, so that they take up less space at the risk of losing quality. SVD allows us to factor our image matrix into the \\(\\textbf{U}\\), \\(\\textbf{D}\\), and \\(\\textbf{V}\\) matrices as defined above. By being selective about how many singular values of \\(\\textbf{D}\\) we want to use, we can approximate the original matrix of our image with a different matrix that will ultimately take up less space. More specifically, if we only wanted to use k singular values, then the corresponding image would only take up \\((n*k + k + k *m) * 8\\) bits of data. The fewer singular values we decide to keep, the more compressed the image will be. By selecting fewer singular values, however, we lose some information about the image, which means that it could become blurrier.\n\n\n\nBelow we have defined the function svd_reconstruct that reconstructs an image from its singular value decomposition (SVD). It requires an image img in greyscale as an argument. It also gives one the options to specify the number of singular values k to use. Alternatively, one can specify a desired compression factor cf, which is the number of bits required to store the compressed image, divided by the number of bits required to store the original image, or a threshold epsilon for the singular values. If one specifies cf, the appropriate k will be calculated for them, and if one specifies epsilon then only the appropriate singular values will be selected for them.\nTo calculate the SVD, we use the np.linalg.svd method and ensure that our k is never larger than n or m.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport warnings \n\n\"\"\"\nGiven a greyscale image (img) reconstruct the image from its singular value decomposition, \nwith the option to specify the number of components (k), a desired compression factor (cf), \nand desired threshold for the singular values (epsilon).\nReturns the img as reconstructed matrix. \n\"\"\"\ndef svd_reconstruct(img, k = 10, cf = 0, epsilon = 0):\n\n    # Get height and width from image \n    m, n = img.shape\n    \n    # Get the img as a 2D matrix \n    data = np.array(img) \n\n    # Calculate k if given compression factor (cf)\n    if (cf != 0):\n        k = int((cf * n * m) / (n + m + 1))\n\n    # k is too large for given image\n    if ((k > m) or (k > n)):\n        warnings.warn(\"WARNING: k > m or k > n of given image\")\n        k = min(n, m)\n    \n    # Compute a singular value decomposition (SVD)\n    U, sigma, V = np.linalg.svd(data)\n\n    # create the D matrix in the SVD\n    D = np.zeros_like(data, dtype=float) # matrix of zeros of same shape as data\n    D[:min(data.shape),:min(data.shape)] = np.diag(sigma) # singular values on the main diagonal\n\n    # User did not specify desired threshold\n    if (epsilon == 0): \n        # Approximate by using specified k components\n        U_ = U[:,:k] # first k columns of U\n        D_ = D[:k, :k] # top k singular values in D\n        V_ = V[:k, :] # first k rows of V\n\n    # User specified a threshold \n    else: \n        # Check how many components have a singular value larger than epsilon \n        new_k = np.count_nonzero(D > epsilon)   \n        \n        \n        # Verify that new_k is within reasonable range\n        if ((new_k > m) or (new_k > n)):\n            new_k = min(n, m)\n\n        # If new_k is smaller than specified components, use new_k specified components \n        if (new_k < k):\n            k = new_k\n            \n        U_ = U[:,:k] # first k columns of U\n        D_ = D[:k, :k] # top k singular values in D\n        V_ = V[:k, :] # first k rows of V \n        \n\n    # Reconstruct and compute approximation of data\n    data_ = U_ @ D_ @ V_\n    \n    # Compute amount of storage the compressed image takes up\n    storage = (n * k) + k + (k * m)\n    fraction = storage / (n * m) \n    \n    return data_, round(fraction, 2)\n\n\n\n\nBefore we run our experiments, we define a compare_svd_reconstruct function that will allow us to more easily see the differences between the original image and a compressed image. This function takes in a greyscale image img and optional arguments that will be passed to svd_reconstruct.\n\n\"\"\"\nGiven an image in greyscale (img), compare it to \nthe compressed image using the provided optional \narguments number of singular values (k), compression factor (cf), \nand threshold of singular values (epsilon). \n\"\"\"\ndef compare_svd_reconstruct(img, k = 10, cf = 0, epsilon = 0):\n    A = np.array(img)\n    A_, storage_percent = svd_reconstruct(A, k, cf = cf, epsilon = epsilon)\n    m, n = A.shape\n    \n    fig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\n    axarr[0].imshow(A, cmap = \"Greys\")\n    axarr[0].axis(\"off\")\n    axarr[0].set(title = \"original image\")\n\n    axarr[1].imshow(A_, cmap = \"Greys\")\n    axarr[1].axis(\"off\")\n    img_title = \"reconstructed image\\n\" + str(storage_percent) + \"% storage\"\n    axarr[1].set(title = img_title)\n    \n    fig.tight_layout()\n\nLet’s see how it does when it only looks at 5 singular values of \\(\\textbf{D}\\).\n\ncompare_svd_reconstruct(to_greyscale(img), 5)\n\n\n\n\nAs discussed earlier, we see that when one selects a “small” number of singular values to use (k), the compressed image doesn’t take up much space. We see here that while it only takes up 0.01% of the original image, the quality of this image is so low to the point were we can’t really make out what the original image was.\nNow let’s run an experiment and reconstruct our image with several different values of k. Our choice of k will go up until we can’t distinguish the reconstructed image from the original by eye.\n\n\"\"\"\nPerform an experiment that reconstructs greyscale image (img) with several different values of k.\n\"\"\"  \ndef svd_experiment(img):\n    rows = 3\n    columns = 4\n    fig, axarr = plt.subplots(rows, columns) \n\n    for i in range(rows):\n        for j in range(columns):\n            k = (i * 4 + j + 1) * 10\n            A_, storage_percent = svd_reconstruct(img, k)\n            img_title = str(k) + \" components\\n\" + str(storage_percent) + \"% storage\"\n            axarr[i][j].imshow(A_, cmap=\"Greys\")\n            axarr[i][j].axis(\"off\")\n            axarr[i][j].set(title=img_title)\n\n    fig.tight_layout()\n\n\nsvd_experiment(to_greyscale(img))\nplt.savefig('experiment.png')\n\n\n\n\nOur final experiment was when k is 120, so let’s see how that compares to our original image.\n\ncompare_svd_reconstruct(to_greyscale(img), 120)\n\n\n\n\nBased on this experiment, its pretty difficult to distinguish which image is the original and which one is reconstructed. Yet we see that the compressed image still saves us a lot of storage.\nIf we continue to experiment with the other arguments of svd_reconstruct, we see that even when SVD uses at all singular values along the diagonal of \\(\\textbf{D}\\), the reconstructed image only uses 1.75% of the space of the original image while still maintaining quality!\n\ncompare_svd_reconstruct(to_greyscale(img), k = min(img.shape[0], img.shape[1]))"
  },
  {
    "objectID": "posts/timnit-gebru/index.html",
    "href": "posts/timnit-gebru/index.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "On April 24th, Dr. Timnit Gebru will be giving a talk and visiting our class virtually for further Q&A on her recent work in AI and tech ethics.\n\n\n\nImage of Timnit Gebru by Cody O’Loughlin from New York Times\n\n\n\n\nDr. Timnit Gebru is a well-known advocate for diversity in technology, who was named one of the World’s 50 Greatest Leaders by Fortune in 2021 and Time’s Most Influential People in 2022.\nWhile working at Microsoft in 2018, Dr. Gebru co-authored a research paper with Joy Buolamwini called Gender Shades. Gender Shades investigated facial recognition technologies from Microsoft, IBM, and Face++ and found that the models did significantly better on lighter skin males than darker skinned females. More recently in 2021, while working at Google Dr. Gebru authored a paper called On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. This paper questioned the ethics of large language AI models and raised concerns about the environmental impact of them. This paper noted that Big Tech companies were neglecting the biases being built into language models, which could exacerbate existing inequalities. It pointed out that Big Tech companies, including Google, were prioritizing profits over safety, and ultimately led to her getting fired by Google.\nToday, Dr. Timnit Gebru serves as the co-founder of Black in AI and founder and leader of Distributed Artificial Intelligence Research Institute (DAIR). She continues to challenge companies to be more thoughtful in their creations and points out the ways in which technology can fail us if not thought about carefully. Through her work, Dr. Gebru pushes us to ask ourselves, “What are we building? Why are we building it? And who is it impacting?”\n\n\n\nIn 2020, Dr. Gebru gave a talk as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision.\nIn this talk, Dr. Gebru discusses the negative effects of image recognition, including its biases, inequalities, and disproportionate harm on various groups and cultures. She emphasizes the impacts that facial recognition can have on our society, highlighting cases such as HireVue’s internal state detection and the Baltimore police’s misuse of facial recognition. She notes that these technologies intended or unintended uses not only further inequities but also infringements on our civil rights.\nDuring her talk, Dr. Gebru highlights that the issue of inequalities in AI and technology is not solely due to the lack of diversity in datasets but also a problem with the system itself.While more diverse datasets can yield better testing results, there must be representation beyond datasets. Dr. Gebru stresses that there must representation from individuals who have been adversely impacted by technology to make decisions about these technologies. In order to address the biases and inequalities in AI technology, we must acknowledge the social and structural issues embedded in technology.\nShe also us urges to remember that no matter how abstract technology may seem, everything is connected to people in some way shape or form. She urges us to be more critical of the automated decisions and reminds us that we need a system that investigates algorithms and their side effects before they are released into the world and whether these tasks are even ethical to begin with.\nTLDR: Due to unrepresentative dataset, systematic problems, and humans innate trust in automated decisions, intended and unintended uses of image recognition is exacerbating inequalities in our society.\n\n\n\nWhat are some effective systems of refusual not just for facial recognition, but other AI technologies?\nSystem that investigates algorithmic bias\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Timnit_Gebru\nhttps://time.com/6132399/timnit-gebru-ai-google/"
  },
  {
    "objectID": "posts/timnit-gebru/index.html#about-dr.-timnit-gebru",
    "href": "posts/timnit-gebru/index.html#about-dr.-timnit-gebru",
    "title": "Learning from Timnit Gebru",
    "section": "About Dr. Timnit Gebru",
    "text": "About Dr. Timnit Gebru\nDr. Timnit Gebru is a well-known advocate for diversity in technology, who was named one of the World’s 50 Greatest Leaders by Fortune in 2021 and Time’s Most Influential People in 2022.\nWhile working at Microsoft in 2018, Dr. Gebru co-authored a research paper with Joy Buolamwini called Gender Shades. Gender Shades investigated facial recognition technologies from Microsoft, IBM, and Face++ and found that the models did significantly better on lighter skin males than darker skinned females. More recently in 2021, while working at Google Dr. Gebru authored a paper called On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. This paper questioned the ethics of large language AI models and raised concerns about the environmental impact of them. This paper noted that Big Tech companies were neglecting the biases being built into language models, which could exacerbate existing inequalities. This paper pointed out that Big Tech companies, including Google, were prioritizing profits over safety, and ultimately led to her getting fired by Google.\nToday, Dr. Timnit Gebru serves the co-founder of Black in AI and founder and leader of Distributed Artificial Intelligence Research Institute (DAIR). She continues to challenge companies to be more thoughtful in their creations and points out the ways in which technology can fail us if not thought about carefully. Through her work, Dr. Gebru pushes us to ask ourselves, “What are we building? Why are we builiding it? And who is it impacting?”"
  },
  {
    "objectID": "posts/timnit-gebru/index.html#dr.-gebrus-talk-at-fate-in-computer-vision",
    "href": "posts/timnit-gebru/index.html#dr.-gebrus-talk-at-fate-in-computer-vision",
    "title": "Learning from Timnit Gebru",
    "section": "Dr. Gebru’s Talk at FATE in Computer Vision",
    "text": "Dr. Gebru’s Talk at FATE in Computer Vision\nIn 2020, Dr. Gebru gave a talk as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision.\nIn this talk, Dr. Gebru emphasizes the biases, inqualities, and disproportional harms image recognition can have on different people and cultures. Think through the implications of technology and who they are supporting and harming and who they are funded by. She pays particular attention to the impacts that facial recogntion can have on our society. She cites HireVue’s internal state detection, Baltimore police’s misuse of facial recognition, and further impacts of detecting one’s gender from their face as examples that do not just deepen inequities, but also infringe on our civil rights. (Whether facial recognition was intended to be used in these ways or not, we are responsible.)\nIn her talk, she points out that these inequalities are not just linked to a lack of diversity in the datasets that are used in training, but also problems in the system. While more diverse datasets can yield more promising testing results, there must be more representation beyond datasets. The teams that make these technology are often only made up of those who are in the dominant group, those who are not harmed by the technologies like women or Black individuals. She empahsizes how technologies can’t just have representation of people in the dominant group, we need representation from people negatively affected by technologies close to making those decisions. When it comes to fixing the biases and inequalities with AI technology, we cannot ignore the social and structural problems.\nShe also urges to remember that no matter how abstract technology may seem, everything is connected to people in some way shape or form. She urges us to be more critical of the automated decisions and reminds us that we need a system that investigates algorithms and their side effects before they are released into the world.\n\nQuestion\nWhat are some effective systems of refusual not just for facial recognition, but other AI technologies? System that investigates algorithmic bias"
  },
  {
    "objectID": "posts/timnit-gebru/index.html#resources",
    "href": "posts/timnit-gebru/index.html#resources",
    "title": "Learning from Timnit Gebru",
    "section": "Resources",
    "text": "Resources\nhttps://en.wikipedia.org/wiki/Timnit_Gebru\nhttps://time.com/6132399/timnit-gebru-ai-google/"
  }
]