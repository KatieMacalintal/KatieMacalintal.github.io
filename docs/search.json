[
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Source Code: perceptron.py\nThe perceptron algorithm is a binary classification algorithm. It works to find a hyperplane that splits the given data into their respective labels. This machine learning algorithm, however, has the limitations that it only works on linearly separable data and data separated only into two groups.\n\n\nIn our Perceptron class, we implemented a fit(X, y) algorithm, which finds the variables of weights, w, that linearly separates X, a data set of observations and their features according to their labels y, such that an element in y is either 0 or 1. In this method, we first start with a random value for w. Then while we haven’t reached a 100% classification rate and have not exceeded the maximum number of steps, we continue to tweak our w depending on if it was able to correctly classify a random point i in our data set X.\nThe update of our weights, w, is seen on line 45 of our perceptron.py code. It is equivalent to the following math equation … \\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y}_i {\\langle \\tilde{w}^{(t)}, \\tilde{x}_i\\rangle} < 0)\\tilde{y}_i \\tilde{x}_i\\]\nThis equation only updates w when our prediction for the label of \\(\\tilde{x}_i\\), the randomly selected data point, is incorrect. This is indicated through the \\(\\mathbb{1}(\\tilde{y}_i {\\langle \\tilde{w}^{(t)}, \\tilde{x}_i\\rangle} < 0)\\) portion of the equation, for it will evaulate to 0 when our w does classify the point correctly and the calcuations will not affect \\(\\tilde{w}^{(t+1)}\\), our w in the next iteration. When the indicator evaulates to 1, meaning our predication was incorrect, then our w is incremented by \\(\\tilde{y}_i \\tilde{x}_i\\). This moves our linear separator in the right direction, so that our randomly selected point could be classified correctly on the next iteration if it were selected.\n\n\n\nBefore we conduct any experiments, we need to import and define relevant extensions, classes, and functions.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron\n\nnp.random.seed(12345)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n\n\nHere is a data set of two distinct groups with 2 features.\n\nn = 100\np_features = 2\n\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(-1.75, -1.75), (1.75, 1.75)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAs shown in the graph, it looks like the points could be separated by a line so that all the purple points, whose label would be “0,” are on one side of the line and all the yellow points, whose label would be “1,” are on the other side of the line.\nWhen we run our Perceptron fit method on this data, we can inspect it’s history of scores. We see that over time the score flucuated as w was adjusted, but it eventually reaches 100% classification. This means that the fit method eventually found the variable of weights that would allow it to create a linear separator on the given data.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nWhen we then graph our line using the weights our Perceptron fit method found, we see that our algorithm was indeed able to separate the data into their respective labels.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nHere is another data set of two distinct groups with 2 features.\n\nn = 100\np_features = 2\n\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(0, 0), (0, 0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThis time, however, it does not look like we can separate the purple and yellow points from each other. Since the data points overlap, it does not look like a linear separator could separate the points into their respective labels.\nWhen looking at this set’s history of scores, we see that it too flucates. This time, however, we see that the score never reaches 1, in other words, it does not reach 100% classification. Instead, it eventually reaches the max number of times we’ll adjust w. While the Perceptron fit method still stores the last value of weights in the object, it communicates that it is not accurate as it gives the warning that the data was unable to converge.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n/Users/katiemacalintal/Desktop/Machine Learning/KatieMacalintal.github.io/posts/perceptron/perceptron.py:51: UserWarning: WARNING: Could not converge\n  warnings.warn(\"WARNING: Could not converge\")\n\n\n\n\n\nWhen we then graph our line using the weights the final iteration of our Perceptron fit method found, we see that, as expected, our algorithm was not able to separate the data into their expected labels. (Note: The scale of the axis may also look different)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nOur perceptron algorithm can also work in more than 2 dimensions. This time we’ve created a data set of two distinct groups with 7 features.\n\nn = 100\np_features = 7\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(-1, -1, -1, -1, -1, -1, -1), (1, 1, 1, 1, 1, 1, 1)])\n\nSince this is difficult to visualize, we will only be inspecting its history of scores. As we can see in the chart, our new data set is linearly separable as the score eventually reaches 1.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nWe can also alter this data set, so that the two groups overlap.\n\nn = 100\np_features = 7\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(10, 10, 10, 10, 10, 10, 10), (10, 10, 10, 10, 10, 10, 10)])\n\nAs we can see by inspecting the history of scores and the warning that is thrown, our data set never converges.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n/Users/katiemacalintal/Desktop/Machine Learning/KatieMacalintal.github.io/posts/perceptron/perceptron.py:51: UserWarning: WARNING: Could not converge\n  warnings.warn(\"WARNING: Could not converge\")\n\n\n\n\n\n\n\n\n\nIn the context of a single iteration of the perceptron algorithm update, I think the runtime complexity of our algorithm would depend on \\(p\\), the number of features. The operation that I think take the longest time would be \\({\\langle \\tilde{w}^{(t)}, \\tilde{x}_i\\rangle}\\), which I think takes \\(O(p)\\) time. There is other multiplication and addition that takes place in this equation, but since it happens consecutively it won’t matter at a larger scale. Thus, I think that runtime of the update would be \\(O(p)\\)."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implement least-squares linear regression and experiment with LASSO regularization for overparameterized problems\n\n\n\n\n\n\nMar 15, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nDetermine the smallest number of measurements necessary to confidently determine the species of a penguin\n\n\n\n\n\n\nMar 8, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplement first-order methods, including simple gradient descent and stochastic gradient descent with momentum, comparing their performance for training logistic regression\n\n\n\n\n\n\nMar 2, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplement and explain the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets\n\n\n\n\n\n\nFeb 22, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Katie Macalintal\nMiddlebury College\nB.A. Computer Science\n2024"
  },
  {
    "objectID": "posts/optimization/index.html",
    "href": "posts/optimization/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Source Code: logistic-regression.py\nIn this blog post, we use logistic regression and gradient descent to efficiently find a hyperplane that can separate a binary classified data set with minimal loss, in other words minimize the empircal risk.\n\n\nIn our LogisticRegression class, we implemented a fit and fit_stochastic function that both take the data set X and their expected labels y.\nIn the fit function, we are looking for the weights w (which includes the bias term) such that it mimimizes our loss. In order to find this w, we use the gradient descent framework with a convex loss function, which combined allows us to search for this local minima. In this framework, we compute the gradient of our loss function: \\[\\ell(\\hat{y}, y)=-y\\log\\sigma(\\hat{y})-(1-y)\\log(1-\\sigma(\\hat{y})),\\] where \\(\\sigma\\) is the logistic sigmoid function and \\(\\hat{y}\\) is our prediction \\(\\langle w,x_i \\rangle\\). This loss function, known as the logistic loss function, is convient for us because it is strictly convex in it’s first argument meaning that our loss can have at most one minimum. The gradient of this loss function turns out to be: \\[\\nabla L(w)=(1/n)\\sum_{i=1}^n (\\sigma(\\hat{y_i})-y_i)x_i.\\] This gradient equation is implemented in the gradient function of our LogisticRegressions class as\nnp.mean(((self.sigmoid(y_) - y)[:,np.newaxis]) * X, axis = 0).\nThen, as stated in Theorem 2 of Optimization with Gradient Descent notes, because our gradent is a descent direction, we adjust our w by stepping in the direction of descent, since we are looking for a w such that our loss is at the lowest it can be. We do this until we either reach the specified max_epochs or converge. In this case, convergence is until the improvement in the our loss function is small enough in magnitiude.\nThe fit_stochastic function is very similar to the fit function, expect here we don’t compute the complete gradient, we instead compute the gradient on a specified batch size. In this function, there is also the option to specify whether one would like to use momentum.\n\n\n\nBefore we conduct any experiments, we need to import and define relevant extensions, classes, and functions.\n\nfrom logistic_regression import LogisticRegression # source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# Graph fitted line and loss function\ndef graph_fit_and_loss(X, y, LR):\n    fig, axarr = plt.subplots(1, 2)\n\n    axarr[0].scatter(X[:,0], X[:,1], c = y)\n    axarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.loss_history[-1]}\")\n\n    f1 = np.linspace(-3, 3, 101)\n\n    p = axarr[0].plot(f1, (- LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\n    axarr[1].plot(LR.loss_history)\n    axarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n    plt.tight_layout()\n\n\n\nWe first create a set of data with 2 features and see that their labels slightly overlap with each other.\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can then fit this data using our fit method, which uses gradient descent, and fit_stochastic, which used batch gradient descent. When we give these fit methods a reasonable learning rate \\(\\alpha\\), then we would expect them to converge.\n\n# Fit the model\nLR_reg = LogisticRegression()\nLR_reg.fit(X, y, alpha = 0.1, max_epochs = 10000)\n\ngraph_fit_and_loss(X, y, LR_reg)\nplt.savefig('gradient_descent.png')\n\n\n\n\n\n# Fit the model\nLR_stoch = LogisticRegression()\nLR_stoch.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10)\n\ngraph_fit_and_loss(X, y, LR_stoch)\n\n\n\n\n\nnum_steps = len(LR_reg.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_reg.loss_history, label = \"gradient\")\n\nnum_steps = len(LR_stoch.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_stoch.loss_history, label = \"stochastic gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nAfter running these methods, we see that the data was able to converge in both cases. We see that they both reach the same loss, but the stochastic gradient descent reaches it in fewer iterations, meaning it iterated over all data points less times. This may be because it makes updates to w more frequently than regular gradient descent does, since it samples its w on smaller portions of data set X and updates it appropriately.\n\n\n\n\nIt’s important that the learning rate \\(\\alpha\\) is relatively small. Before we set our alpha to 0.1, but if we use the same data and set \\(\\alpha\\) to a too high number we will see that we never converge in both regular gradient descent and stochastic gradient descent. If \\(\\alpha\\) too large, we might be updating w by too much such that it overshoots where the minimum actually is resulting in a flucuating loss history.\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 170, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n/Users/katiemacalintal/Desktop/Machine Learning/KatieMacalintal.github.io/posts/optimization/logistic_regression.py:54: UserWarning: WARNING: Could not converge\n  warnings.warn(\"WARNING: Could not converge\")\n\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 75, max_epochs = 1000, batch_size = 50)\n\ngraph_fit_and_loss(X, y, LR)\n\n/Users/katiemacalintal/Desktop/Machine Learning/KatieMacalintal.github.io/posts/optimization/logistic_regression.py:119: UserWarning: WARNING: Could not converge\n  warnings.warn(\"WARNING: Could not converge\")\n\n\n\n\n\n\nLR.fit(X, y, alpha = 75, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\nThrough graphing the loss history, we see that it fluctuates greatly due to the large \\(\\alpha\\) size. We also see that because stochastic gradient descent updates w more often, our fit_stochastic doesn’t converge with a high learning rate that our regular gradient descent method can actually converge with.\n\n\n\nNow, we will run some experiments on how the batch size influences convergence. We will do this with a larger data set of 1000 points, which have 10 features.\n\n# Make the data\np_features = 11\nX, y = make_blobs(n_samples = 1000, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\ndef graph_loss(LR):\n    fig, axarr = plt.subplots(1, 1)\n\n    axarr.plot(LR.loss_history)\n    axarr.set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = f\"Loss = {LR.loss_history[-1]}\")\n    plt.tight_layout()\n\nIn the Disenroth, Faisal, and Soon reading, they observe that “Large mini-batch sizes will provide accurate estimates of the gradient, reducing the variance in the parameter update. … The reduction in variance leads to more stable convergence, but each graident calculation will be more expensive” (232). On the other hand, they observe that “small mini-batches are quick to estimate. If we keep the mini-batch size small, the noise in our graident estimate will allow us to get out of some bad local optima” (232). We’ll conduct experiments such that our batch_size decreases in size, and we’ll inspect their loss to see how Disenroth, Faisal, and Soon’s observations hold.\n\nLR_500 = LogisticRegression()\nLR_500.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 500)\n\ngraph_loss(LR_500)\n\n\n\n\n\nLR_50 = LogisticRegression()\nLR_50.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 50)\n\ngraph_loss(LR_50)\n\n\n\n\n\nLR_25 = LogisticRegression()\nLR_25.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 25)\n\ngraph_loss(LR_25)\n\n\n\n\n\nLR_5 = LogisticRegression()\nLR_5.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 5)\n\ngraph_loss(LR_5)\n\n\n\n\n\nnum_steps = len(LR_500.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_500.loss_history, label = \"batch size 500\")\n\nnum_steps = len(LR_50.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_50.loss_history, label = \"batch size 50\")\n\nnum_steps = len(LR_25.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_25.loss_history, label = \"batch size 25\")\n\nnum_steps = len(LR_5.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_5.loss_history, label = \"batch size 5\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nWe see that as our batch size gets smaller, our number of iterations over the all data points in X also gets smaller. This relates to why stochastic gradient descent uses fewer iterations as gradient descent. If our batch size is large, we do not update w as frequently as we would have if it were smaller. Based on our loss graphs, we also see that, as noted by Disenroth, Faisal, and Soon, stochastic gradient descent with large batch sizes have a smooth convergence, and small batch sizes have a noiser convergence.\n\n\n\nIn the fit_stochastic function we also implemented the option to use momentum, which takes into account our previously taken step and allows us to continue moving along that direction if the update was good. In the case shown below, we see that momentum was able to result in less iterations over the data, but understanding when exactly momentum would significantly speed up convergence will require more experiments.\n\n# Make the data\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (0, 0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nLR = LogisticRegression()\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = False)\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = True)\ngraph_fit_and_loss(X, y, LR)"
  },
  {
    "objectID": "posts/optimization/momentum_experiments/index.html",
    "href": "posts/optimization/momentum_experiments/index.html",
    "title": "CSCI 0451 Blog",
    "section": "",
    "text": "In this function, we also can use momentum, which takes into account our previously taken step and allows us to continue moving along that direction if the update was good."
  },
  {
    "objectID": "posts/optimization/momentum_experiments/index.html#momentum",
    "href": "posts/optimization/momentum_experiments/index.html#momentum",
    "title": "CSCI 0451 Blog",
    "section": "Momentum",
    "text": "Momentum\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nLR = LogisticRegression()\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = False)\ngraph_fit_and_loss(X, y, LR)\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = True)\ngraph_fit_and_loss(X, y, LR)\n\nDifficult to tell that momentum makes a difference here.\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (0, 0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = False)\ngraph_fit_and_loss(X, y, LR)\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = True)\ngraph_fit_and_loss(X, y, LR)"
  },
  {
    "objectID": "posts/Untitled Folder/index.html",
    "href": "posts/Untitled Folder/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Source Code: logistic-regression.py\nIn this blog post, we use logistic regression and gradient descent to efficiently find a hyperplane that can separate a binary classified data set with minimal loss, in other words minimize the empircal risk.\n\n\nIn our LogisticRegression class, we implemented a fit and fit_stochastic function that both take the data set X and their expected labels y.\nIn the fit function, we are looking for the weights w (which includes the bias term) such that it mimimizes our loss. In order to find this w, we use the gradient descent framework, which searches for this local minima. In this framework, we compute the gradient of our loss function: \\[\\ell(\\hat{y}, y)=-y\\log\\sigma(\\hat{y})-(1-y)\\log(1-\\sigma(\\hat{y})),\\] where \\(\\sigma\\) is the logistic sigmoid function and \\(\\hat{y}\\) is our prediction \\(\\langle w,x_i \\rangle\\). This loss function, known as the logistic loss function, is convient for us because it is strictly convex in it’s first argument meaning that our loss can have at most one minimum. The gradient of this loss function turns out to be: \\[\\nabla L(w)=(1/n)\\sum_{i=1}^n (\\sigma(\\hat{y_i})-y_i)x_i.\\] This gradient equation is implemented in the gradient function of the code like\nnp.mean(((self.sigmoid(y_) - y)[:,np.newaxis]) * X, axis = 0).\nThen, as stated in Theorem 2 of Optimization with Gradient Descent notes, because our gradent is a descent direction, we adjust our w by stepping in the direction of descent since we are looking for a w such that our loss is at the lowest it can be. We do this until we either reach the specified max_epochs or convergence. In this case, convergence is until the improvement in the our loss function is small enough in magnitiude.\nThe fit_stochastic function is very similar to the fit function, expect this time we don’t compute the complete gradient, we instead compute the gradient on a batch size.\n\n\n\nBefore we conduct any experiments, we need to import and define relevant extensions, classes, and functions.\n\nfrom logistic_regression import LogisticRegression # source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\ndef graph_fit_and_loss(X, y, LR):\n    fig, axarr = plt.subplots(1, 2)\n\n    axarr[0].scatter(X[:,0], X[:,1], c = y)\n    axarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.loss_history[-1]}\")\n\n    f1 = np.linspace(-3, 3, 101)\n\n    p = axarr[0].plot(f1, (- LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\n    axarr[1].plot(LR.loss_history)\n    axarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n    plt.tight_layout()\n\n\n\nWe first created a set of data with 2 features and see that their labels slightly overlaps with each other.\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can then fit this data using our fit method, which uses gradient descent, and fit_stochastic, which used batch gradient descent. When we give these fit methods a reasonable learning rate \\(\\alpha\\), then we would expect them to converge.\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 10000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\nAfter running these methods, we see that the data was able to converge in both cases. We see that they both reach the same loss, but the stochastic gradient descent reaches it in fewer iterations, meaning it iterated over all data points less times. This may be because it makes updates to w more frequently than regular gradient descent does, since it samples its w on smaller portions of data set X and updates as it goes.\n\n\n\n\nIt’s important that the learning rate \\(\\alpha\\) is relatively small. Before we set our alpha to 0.1, but if we use the same data and set it to a too high number we will see that we never converge in both regular gradient descent and stochastic gradient descent. If it’s too large, we might be updating w by too much such that it eventually skips over the minimum.\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 125, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 50, max_epochs = 1000, batch_size = 50)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\nLR.fit(X, y, alpha = 50, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\nWe see, however, that because stochastic gradient descent updates w more often depending on our batch_size, our fit_stochastic doesn’t converge with a high learning rate that our regular gradient descent method can converge with.\n\n\n\nNow, we will run some experiments on how the batch size influences convergence. We will do this with a larger data set that has 10 features.\n\n# Make the data\np_features = 11\nX, y = make_blobs(n_samples = 1000, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\ndef graph_loss(LR):\n    fig, axarr = plt.subplots(1, 1)\n\n    axarr.plot(LR.loss_history)\n    axarr.set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = f\"Loss = {LR.loss_history[-1]}\")\n    plt.tight_layout()\n\nIn the Disenroth, Faisal, and Soon reading, they observe that “Large mini-batch sizes will provide accurate estimates of the gradient, reducing the variance in the parameter update. … The reduction in variance leads to more stable convergence, but each graident calculation will be more expensive” (232) On the other hand, they observe that “small mini-batches are quick to estimate. If we keep the mini-batch size small, the noise in our graident estimate will allow us to get out of some bad local optima” (232). We’ll conduct experiments such that our batch_size decreases in size, and we’ll inspect their loss to see how Disenroth, Faisal, and Soon’s observations hold.\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 500)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 50)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 25)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 5)\n\ngraph_loss(LR)\n\n\n\n\nWe see that as our batch size gets smaller, our number of iterations over the all data points in X also gets smaller. This relates to why stochastic gradient descent uses fewer iterations as gradient descent. As our batch size gets larger, we do not updates w as frequently as we would have if it were smaller. However, stochastic gradient descent with large batch sizes"
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Chinstrap, Gentoo, and Adelie penguin clipart"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#data-preparation",
    "href": "posts/palmer-penguins/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n# species = [s.split()[0] for s in le.classes_]\n\n# Prepare qualitative data and mark species as labels\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df)\n    return df, y\n\n# Prepare training data\nX_train, y_train = prepare_data(train)\n# X_train.head()"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#question",
    "href": "posts/palmer-penguins/index.html#question",
    "title": "Classifying Palmer Penguins",
    "section": "Question",
    "text": "Question\nPredict Question: Can we predict the species of a penguin given information on their bodies, etc?\nQUESTIONS ABOUT HOW THE FEATURES INFLUENCE THE LABEL\nLook at the features and ask questions about whether they influence what label they will be:\n\nimport seaborn as sns\nsns.set_theme()\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\"\n)\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y='Flipper Length (mm)', hue=\"Species\"\n)\n\n<seaborn.axisgrid.FacetGrid at 0x13779fdc0>"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#choosing-features",
    "href": "posts/palmer-penguins/index.html#choosing-features",
    "title": "Classifying Palmer Penguins",
    "section": "Choosing Features",
    "text": "Choosing Features\nMight need combinations function from the itertools package\nUSE CROSS-VALIDATION! Simplest way to guard against overfitting issues and get a good feeling for how your model might do on unseen data.\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\nall_qual_cols = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\n# Resource: https://www.datatechnotes.com/2021/02/seleckbest-feature-selection-example-in-python.html\n\n# Pick qualatative features\nX_qual = X_train[all_qual_cols]\nqual_selected = SelectKBest(f_classif, k=3).fit(X_qual, y_train)\nmask = qual_selected.get_support()\nqual_names = X_qual.columns[mask]\nselected_qual = X_train[qual_names]\nprint(qual_names)\n\n# Pick quantatative features \nX_quant = X_train[all_quant_cols]\nquant_select = SelectKBest(f_classif, k=2).fit(X_quant, y_train)\nmask = quant_select.get_support()\nquant_names = X_quant.columns[mask]\nselected_quant = X_train[quant_names]\nown_quant = np.array(['Culmen Length (mm)', 'Culmen Depth (mm)'])\nprint(quant_names)\n\nfeatures = np.concatenate((quant_names, qual_names))\nprint(features)\n\n# Combine them after inspecting \n# X_selected_features = pd.concat([selected_quant, selected_qual], axis=1)\n# X_selected_features\n\nIndex(['Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], dtype='object')\nIndex(['Culmen Length (mm)', 'Flipper Length (mm)'], dtype='object')\n['Culmen Length (mm)' 'Flipper Length (mm)' 'Island_Biscoe' 'Island_Dream'\n 'Island_Torgersen']\n\n\n\n#FIND MAX DEPTH OF TREE \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nfig, ax = plt.subplots(1)\n\nmax_score = 0\nbest_depth = 0\nfor d in range(2, 10):\n    # USING DECISION TREE CLASSIFER \n    T = DecisionTreeClassifier(max_depth = d)\n    cv_mean = cross_val_score(T, X_train[features], y_train, cv = 10).mean()\n    ax.scatter(d, cv_mean, color = \"black\")\n    if cv_mean > max_score:\n        max_score = cv_mean \n        best_depth = d\n\nlabs = ax.set(xlabel = \"Complexity (depth)\", ylabel = \"Performance (score)\")\n       \nprint(max_score, best_depth)\n\n0.9766153846153847 5\n\n\n\n\n\n\n#FIND GAMMA \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\n\nfig, ax = plt.subplots(1)\n\nmax_score = 0\nbest_gamma = 0\nfor g in range(1, 10):\n    little_g = g / 100\n    # USING DECISION TREE CLASSIFER \n    svc = SVC(gamma = little_g, kernel = \"rbf\")\n    cv_mean = cross_val_score(svc, X_train[features], y_train, cv = 10).mean()\n    ax.scatter(little_g, cv_mean, color = \"black\")\n    if cv_mean > max_score:\n        max_score = cv_mean \n        best_gamma = little_g\n\nlabs = ax.set(xlabel = \"Gamma\", ylabel = \"Performance (score)\")\n       \nprint(max_score, best_gamma)\n\n0.9572307692307692 0.03\n\n\n\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef decision_region_panel(X, y, model, qual_features):  \n    p = len(qual_features)\n    fig, axarr = plt.subplots(1, p, figsize=(4*p,4))\n    for i in range(p):\n\n        filler_feature_values = {2+j: 0 for j in range(p)}\n        \n        filler_feature_values.update({2+i: 1})\n        \n        ix = X[qual_features[i]] == 1\n\n        ax = axarr[i]\n        # print(f\"{X[ix]=}\")\n        plot_decision_regions(np.array(X[ix]), y[ix], clf=model,\n                            filler_feature_values=filler_feature_values,\n                            filler_feature_ranges={2+j: 0.1 for j in range(p)},\n                            legend=2, ax=ax)\n\n        ax.set_xlabel(X.columns[0])\n        ax.set_ylabel(X.columns[1])\n\n        handles, labels = ax.get_legend_handles_labels()\n        ax.legend(handles, \n          [\"Adelie\", \"Chinstrap\", \"Gentoo\"],\n           framealpha=0.3, scatterpoints=1)\n        \n    # Adding axes annotations\n    fig.suptitle(f'Accuracy = {model.score(X, y).round(3)}')\n    plt.tight_layout()\n    plt.show()\n\nDTC = DecisionTreeClassifier(max_depth = best_depth)\nDTC.fit(X_train[features], y_train)\n# print(DTC.score(X_train[features], y_train))\n# decision_region_panel(X_train[features], y_train, DTC, qual_names)\n\nsvc = SVC(gamma = best_gamma) \nsvc.fit(X_train[features], y_train)\n# print(svc.score(X_train[features], y_train))\n# decision_region_panel(X_train[features], y_train, svc, qual_names)\n\nLR = LogisticRegression()\nLR.fit(X_train[features], y_train)\n# print(LR.score(X_train[features], y_train))\nprint(f\"features: {features}\")\nprint(f\"qual_names: {qual_names}\")\ndecision_region_panel(X_train[features], y_train, LR, qual_names)\n\nfeatures: ['Culmen Length (mm)' 'Flipper Length (mm)' 'Island_Biscoe' 'Island_Dream'\n 'Island_Torgersen']\nqual_names: Index(['Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], dtype='object')\n\n\n\n\n\n\n# TESTING\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\nX_test, y_test = prepare_data(test)\n\n\nprint(DTC.score(X_test[features], y_test))\nprint(svc.score(X_test[features], y_test))\nprint(LR.score(X_test[features], y_test))\n# YOU GET 100% TESTING ACCURACY WITH QUANTATIVE culmen length and culmen depth \n\n\n# NOT USING \nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n\n# all_qual_cols = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\"]\nall_qual_cols = [\"Island\", \"Clutch\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\n# Create dataframe to better inspect the scores\npd.set_option('max_colwidth', 100)\nscores_df = pd.DataFrame(columns=['Columns', 'Score'])\n\n# Go through possible combinations of features and train model on them \n#     Using 1 qualitative and 2 quantiative \nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col ]\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n        # print(cols)\n        # Using logistic regression for modeling \n        LR = LogisticRegression()\n        # Incorportating cross validation? \n        cv_scores = cross_val_score(LR, X_train[cols], y_train, cv=10)\n        mean_score = cv_scores.mean()\n        scores_df = scores_df.append({'Columns': cols, 'Score': mean_score.round(3)}, ignore_index=True)\n\nscores_df = scores_df.sort_values(by='Score', ascending=False).reset_index(drop=True)\nscores_df\n    \n    #LR.fit(X_train[cols], y_train)\n    #LR.score(X_train[cols], y_train)\n    # print(f\"Features: {cols} \\n\\tscore: {mean_score.round(3)}\")\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#model-choices",
    "href": "posts/palmer-penguins/index.html#model-choices",
    "title": "Classifying Palmer Penguins",
    "section": "Model Choices",
    "text": "Model Choices\nRemember: We are working with 3 label now not 2.\n\n# from sklearn.linear_model import LogisticRegression\n\n# # this counts as 3 features because the two Clutch Completion \n# # columns are transformations of a single original measurement. \n# # you should find a way to automatically select some better columns\n# # as suggested in the code block above\n# cols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\n# LR = LogisticRegression()\n# LR.fit(X_train[cols], y_train)\n# LR.score(X_train[cols], y_train)\n\nSince scikit-learn makes it so easy to experiment, this blog post is a great opportunity to explore some out-of-the-box models that we haven’t discussed in class. I’d suggest:\n\nfrom sklearn.tree import DecisionTreeClassifier. This one has a max_depth parameter that controls the complexity of the model. Use cross-validation to find a good value of the parameter.\nfrom sklearn.ensemble import RandomForestClassifier. State-of-the-art before the rise of neural networks.\nfrom sklearn.svm import SVC. Another state-of-the-art algorithm before the rise of neural networks. Has a parameter gamma that controls the complexity of the model. Again, use cross-validation to select gamma. It’s important to let gamma cover a wide range of values, e.g. gamma = 10**np.arange(-5, 5).\n\nYou can find a more thorough listing of models on this page."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#need-to-test-and-plot-decision-regions",
    "href": "posts/palmer-penguins/index.html#need-to-test-and-plot-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "NEED TO TEST AND PLOT DECISION REGIONS",
    "text": "NEED TO TEST AND PLOT DECISION REGIONS\nHas a link to use for testing later"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#download-training-data",
    "href": "posts/palmer-penguins/index.html#download-training-data",
    "title": "Classifying Palmer Penguins",
    "section": "Download Training Data",
    "text": "Download Training Data\nFirst, we download our given training data.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load training data\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#plotting-decision-regions",
    "href": "posts/palmer-penguins/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting Decision Regions",
    "text": "Plotting Decision Regions\nCode provided with …\n\nqual_features = [\"Clutch Completion_No\", \"Clutch Completion_Yes\"]\ndecision_region_panel(X_train[cols], y_train, LR, qual_features)\n\nKeyError: 'Clutch Completion_No'"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#prepare-training-data",
    "href": "posts/palmer-penguins/index.html#prepare-training-data",
    "title": "Classifying Palmer Penguins",
    "section": "Prepare Training Data",
    "text": "Prepare Training Data\nNext, we tidy up our data. We remove any columns that are irrelevant to determining the species of a penguin and modify any qualitative features (e.g. sex, clutch completion, island), so that they are represented through numerical values instead of strings, since strings are difficult to quantify and work with.\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n\"\"\"\nPrepare qualitative data and mark species as labels\n\"\"\"\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df)\n    return df, y\n\n# Prepare training data\nX_train, y_train = prepare_data(train)"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#explore-feature-selection",
    "href": "posts/palmer-penguins/index.html#explore-feature-selection",
    "title": "Classifying Palmer Penguins",
    "section": "Explore: Feature Selection",
    "text": "Explore: Feature Selection\nNow that we have prepared our training data, we want to figure our which three features of the data (two quantitative and one qualitative) will allow a model to achieve 100% testing accuracy when trained on those features.\nThe first way in which we tried to select these features was through the SelectKBest and f_classif functions in the sklearn.feature_selection package.\n\n# Resource: https://www.datatechnotes.com/2021/02/seleckbest-feature-selection-example-in-python.html\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nall_qual_cols = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\n# Pick quantatative features \nX_quant = X_train[all_quant_cols]\nquant_select = SelectKBest(f_classif, k=2).fit(X_quant, y_train)\nmask = quant_select.get_support()\nquant_names = X_quant.columns[mask]\n\n# Pick qualatative features\nX_qual = X_train[all_qual_cols]\nqual_selected = SelectKBest(f_classif, k=3).fit(X_qual, y_train)\nmask = qual_selected.get_support()\nqual_names = X_qual.columns[mask]\n\nfeatures = np.concatenate((quant_names, qual_names))\n\n\nprint(f\"quant_names: {quant_names}\")\nprint(f\"qual_names: {qual_names}\")\nprint(f\"features: {features}\")\n\nquant_names: Index(['Culmen Length (mm)', 'Flipper Length (mm)'], dtype='object')\nqual_names: Index(['Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], dtype='object')\nfeatures: ['Culmen Length (mm)' 'Flipper Length (mm)' 'Island_Biscoe' 'Island_Dream'\n 'Island_Torgersen']\n\n\nWhen we inspect the features SelectKBest chose based on the f_classif score function, we see that it found the quantative Culmen Length (mm) and Flipper Length (mm) features and qualitative Island feature to be our most useful features with the highest scores.\nSince our data doesn’t have too many features, another way in which we could have selected features was through an exhaustive search that uses the combinations function from the itertools package. To guard ourselves from overfitting issues, we use cross validation throughout this process with LogisticRegression as our model.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nall_qual_cols = [\"Island\", \"Clutch\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\n# Create dataframe to better inspect the scores\npd.set_option('max_colwidth', 100)\nscores_df = pd.DataFrame(columns=['Columns', 'Score'])\n\n# Go through possible combinations of features and train model on them \n#     Using 1 qualitative and 2 quantiative \nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col ]\n    for pair in combinations(all_quant_cols, 2):\n        cols = list(pair) + qual_cols \n        # Using logistic regression for modeling \n        LR = LogisticRegression()\n        # Incorportating cross validation\n        cv_mean_score = cross_val_score(LR, X_train[cols], y_train, cv=10).mean()\n        scores_df = scores_df.append({'Columns': cols, 'Score': cv_mean_score.round(3)}, ignore_index=True)\n\nscores_df = scores_df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n\n\nfeatures = scores_df.iloc[0,0]\nfeatures\n\n['Culmen Length (mm)',\n 'Culmen Depth (mm)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']\n\n\nWe see that the exhaustive search also found the qualitative Island feature to be most useful. We can further inspect why this qualitative Island feature was chosen over Sex and Clutch Completion using functions like groupby and aggregate from the pandas package.\n\n\"\"\"\nResources: \nhttps://jakevdp.github.io/PythonDataScienceHandbook/03.08-aggregation-and-grouping.html\nhttps://www.geeksforgeeks.org/python-pandas-dataframe-reset_index/\nhttps://towardsdatascience.com/interesting-ways-to-select-pandas-dataframe-columns-b29b82bbfb33\nhttps://sites.ualberta.ca/~hadavand/DataAnalysis/notebooks/Reshaping_Pandas.html\n\"\"\"\n\n# Group the penguins by species and island, and count the number of occurrences\ncounts = train.groupby(['Species', 'Island']).size().reset_index(name='count')\n\n# Group the penguins by island and compute the total count for each island\nisland_totals = counts.groupby('Island')['count'].sum().reset_index(name='total')\n\n# Merge the counts and island_totals\nresults = pd.merge(counts, island_totals, on='Island')\n\n# Compute the percentage\nresults['percentage'] = results['count'] / results['total'] * 100\n\n# Edit results dataframe so that it only contains the Island, Species and percentage\nresults = results[['Island', 'Species', 'percentage']]\n\n# Arrange results to have islands as columns and species as rows\nresults = results.pivot(index='Species', columns='Island', values='percentage').round(2)\n\nprint(results)\n\nIsland                                     Biscoe  Dream  Torgersen\nSpecies                                                            \nAdelie Penguin (Pygoscelis adeliae)         25.74  42.27      100.0\nChinstrap penguin (Pygoscelis antarctica)     NaN  57.73        NaN\nGentoo penguin (Pygoscelis papua)           74.26    NaN        NaN\n\n\n\ncounts = train.groupby(['Species', 'Sex']).size().reset_index(name='count')\nsex_totals = counts.groupby('Sex')['count'].sum().reset_index(name='total')\nresults = pd.merge(counts, sex_totals, on='Sex')\nresults['percentage'] = results['count'] / results['total'] * 100\nresults = results[['Sex', 'Species', 'percentage']]\nresults = results.pivot(index='Species', columns='Sex', values='percentage').round(2)\nresults = results.drop(columns='.')\n\nprint(results)\n\nSex                                        FEMALE   MALE\nSpecies                                                 \nAdelie Penguin (Pygoscelis adeliae)         44.53  40.44\nChinstrap penguin (Pygoscelis antarctica)   22.66  19.85\nGentoo penguin (Pygoscelis papua)           32.81  39.71\n\n\n\ncounts = train.groupby(['Species', 'Clutch Completion']).size().reset_index(name='count')\nclutch_totals = counts.groupby('Clutch Completion')['count'].sum().reset_index(name='total')\nresults = pd.merge(counts, clutch_totals, on='Clutch Completion')\nresults['percentage'] = results['count'] / results['total'] * 100\nresults = results[['Clutch Completion', 'Species', 'percentage']]\nresults = results.pivot(index='Species', columns='Clutch Completion', values='percentage').round(2)\n\nprint(results)\n\nClutch Completion                             No    Yes\nSpecies                                                \nAdelie Penguin (Pygoscelis adeliae)        37.93  43.50\nChinstrap penguin (Pygoscelis antarctica)  37.93  18.29\nGentoo penguin (Pygoscelis papua)          24.14  38.21\n\n\nWhen we inspect the qualitative features in this way, we see that each island has at most two different penguin species that live there. For the Sex and Clutch Completion features, however, we see that it’s harder to differentiate the species based on those values for it could be any of the three species.\nLooking back at our exhaustive search feature results, we also see, however, that it found a different pair of quantatative features with a higher sore. It found Culmen Depth (mm) and Culmen Length to be more useful features.\nTo figure out whether the SelectKBest Flipper Length (mm) and Culmen Length features or exhaustive search features Culmen Depth (mm) and Culmen Length (mm) are the better pair of quantitative features, let’s inspect what they look like when graphed using the seaborn package.\n\nimport seaborn as sns\nsns.set_theme()\n\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y='Flipper Length (mm)', hue=\"Species\"\n).set(title = \"SelectKBest Features\")\n\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\"\n).set(title = \"Exhaustive Search Features\")\n\n<seaborn.axisgrid.FacetGrid at 0x157babbb0>\n\n\n\n\n\n\n\n\nBased on these graphs, it looks like Culmen Depth (mm) and Culmen Length (mm) may be the better quantative options for it looks like they have less overlap among their species. In other words, it’s more easily separable and distinguishable.\nTo summarize when training our models, we will use the quantitative Culmen Depth and Culmen Length features and qualitative Island feature."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#explore-modeling",
    "href": "posts/palmer-penguins/index.html#explore-modeling",
    "title": "Classifying Palmer Penguins",
    "section": "Explore: Modeling",
    "text": "Explore: Modeling\nNow that we have chosen our features, we can begin to train different models using our training data. In this blog post, we explore the models of DecisionTreeClassifier, RandomForestClassifier, and LogisticRegression and use the plot_regions method below to visualize our decision regions.\n\nfrom matplotlib.patches import Patch\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n\n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n        XY = pd.DataFrame({\n            X.columns[0] : XX,\n            X.columns[1] : YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n      \n        # use contour plot to visualize the predictions\n        axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n\n        ix = X[qual_features[i]] == 1\n        # plot the data\n        axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n\n        axarr[i].set(xlabel = X.columns[0], \n                     ylabel  = X.columns[1])\n        axarr[i].set_title(qual_features[i])\n\n        patches = []\n        for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n            patches.append(Patch(color = color, label = spec))\n        \n        plt.suptitle(f\"Score {model.score(X, y)}\")\n        plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n        plt.tight_layout()\n\n\nDecisionTreeClassifier\nFor the DecisionTreeClassifer model, we need to provide it with a max_depth argument, which helps control the complexity of the model. In order to find a good max_depth value, we use cross validation to help prevent overfitting.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nfig, ax = plt.subplots(1)\n\nmax_score = 0\nbest_depth = 0\nfor d in range(2, 10):\n    T = DecisionTreeClassifier(max_depth = d)\n    cv_mean = cross_val_score(T, X_train[features], y_train, cv = 10).mean()\n    ax.scatter(d, cv_mean, color = \"black\")\n    if cv_mean > max_score:\n        max_score = cv_mean \n        best_depth = d\n\nlabs = ax.set(xlabel = \"Complexity (depth)\", ylabel = \"Performance (score)\")\n\n\n\n\nNow that we have found the most suitable max_depth, we can train our model with it and look at its decision regions.\n\nDTC = DecisionTreeClassifier(max_depth = best_depth)\nDTC.fit(X_train[features], y_train)\nplot_regions(DTC, X_train[features], y_train)\n\n\n\n\nBased on these plotted decision regions and the score, it looks like our DecisionTreeClassifier model did a good job with correctly classifying our training data. It also looks like it was able to do it without overfitting, for the graphs do not look too wiggly or tailored so much to our training data.\n\n\nRandomForestClassifier\nFor the RandomForestClassifier model, no arguments are required. So we can just train our model with our selected features.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier()\nRFC.fit(X_train[features], y_train)\nplot_regions(RFC, X_train[features], y_train)\n\n\n\n\nAgain, based on these plotted decision regions and the score, it looks like our model did a good job with correctly classifying our training data. It does, however, look like it may be overfitting the data for some of the decision regions look a bit wiggly and tailored too much towards our training data.\n\n\nLogisticRegression\nFor the LogisticRegression model, no arguments are also required. So we can just train our model with our selected features.\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\nLR.fit(X_train[features], y_train)\nplot_regions(LR, X_train[features], y_train)\n\n\n\n\nAgain, based on these plotted decision regions and the score, it looks like our model did a great job with correctly classifying our training data, even better than the DecisionTreeClassifier model. It also looks like it was able to do it without overfitting, for the graphs do not look too wiggly or tailored so much to our training data."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#testing",
    "href": "posts/palmer-penguins/index.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "Testing",
    "text": "Testing\nNow, that we have trained one model using DecisionTreeClassifer and the other using LogisticRegression, we can see which one will yield us our desired results of 100% testing accurac\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\nX_test, y_test = prepare_data(test)\n\n\nprint(DTC.score(X_test[features], y_test))\nprint(LR.score(X_test[features], y_test))\n# YOU GET 100% TESTING ACCURACY WITH QUANTATIVE culmen length and culmen depth \n# Show graphs\n\n0.9852941176470589\n1.0"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#results",
    "href": "posts/palmer-penguins/index.html#results",
    "title": "Classifying Palmer Penguins",
    "section": "Results",
    "text": "Results\n\n#FIND GAMMA \n# from sklearn.model_selection import cross_val_score\n# from sklearn.svm import SVC\n\n# fig, ax = plt.subplots(1)\n\n# max_score = 0\n# best_gamma = 0\n# for g in range(1, 100000):\n#     little_g = g / 100000\n#     # USING DECISION TREE CLASSIFER \n#     svc = SVC(gamma = little_g, kernel = \"rbf\")\n#     cv_mean = cross_val_score(svc, X_train[features], y_train, cv = 10).mean()\n#     ax.scatter(little_g, cv_mean, color = \"black\")\n#     if cv_mean > max_score:\n#         max_score = cv_mean \n#         best_gamma = little_g\n\n# labs = ax.set(xlabel = \"Gamma\", ylabel = \"Performance (score)\")\n       \n# print(max_score, best_gamma)\n\n# svc = SVC(gamma = best_gamma) \n# svc.fit(X_train[features], y_train)\n# # print(svc.score(X_train[features], y_train))\n# # decision_region_panel(X_train[features], y_train, svc, qual_names)\n# plot_regions(svc, X_train[features], y_train)\n\nKeyboardInterrupt: \n\n\nError in callback <function flush_figures at 0x14cb143a0> (for post_execute):\n\n\nKeyboardInterrupt:"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#testing-and-results",
    "href": "posts/palmer-penguins/index.html#testing-and-results",
    "title": "Classifying Palmer Penguins",
    "section": "Testing and Results",
    "text": "Testing and Results\nNow, that we have models trained using DecisionTreeClassifer, RandomForestClassifier, and LogisticRegression, we can see which one will yield us our desired results of 100% testing accuracy.\nFirst, we download and prepare our testing data.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\nX_test, y_test = prepare_data(test)\n\nNext, we can inspect the performance of each model on the testing data by plotting their decision regions.\n\nplot_regions(DTC, X_test[features], y_test)\n\n\n\n\n\nplot_regions(RFC, X_test[features], y_test)\n\n\n\n\n\nplot_regions(LR, X_test[features], y_test)\n\n\n\n\nAs we can see based off our decision regions and scores, while our DecisionTreeClassifer and RandomForestClassifier model yields promising classification, our LogisticRegression model trained on the features Culmen Length, Culmen Depth, and Islands does even better as it results in 100% testing accuracy."
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "Source Code: linear_regression.py"
  },
  {
    "objectID": "posts/linear-regression/index.html#experiments",
    "href": "posts/linear-regression/index.html#experiments",
    "title": "Implementing Linear Regression",
    "section": "Experiments",
    "text": "Experiments\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR = LinearRegression()\n    LR.fit_analytic(X_train, y_train)\n    training_scores.append(LR.score(pad(X_train), y_train))\n    validation_scores.append(LR.score(pad(X_val), y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.yscale('log')\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")"
  },
  {
    "objectID": "posts/linear-regression/index.html#lasso",
    "href": "posts/linear-regression/index.html#lasso",
    "title": "Implementing Linear Regression",
    "section": "LASSO",
    "text": "LASSO\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nL.score(X_val, y_val)\n\n0.7548622009585426"
  },
  {
    "objectID": "posts/linear-regression/index.html#lasso-1",
    "href": "posts/linear-regression/index.html#lasso-1",
    "title": "Implementing Linear Regression",
    "section": "LASSO",
    "text": "LASSO\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    L = Lasso(alpha = 0.0009)\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.yscale('log')\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\n\n\n\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    L = Lasso(alpha = 0.001)\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.yscale('log')\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\n\n\n\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    L = Lasso(alpha = 0.01)\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.yscale('log')\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\n\n\n\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    L = Lasso(alpha = 0.1)\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.yscale('log')\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")"
  },
  {
    "objectID": "posts/linear-regression/index.html#implementation",
    "href": "posts/linear-regression/index.html#implementation",
    "title": "Implementing Linear Regression",
    "section": "Implementation",
    "text": "Implementation\nResource: Regression Notes\nAs discussed in this course, least-squares linear regression fits nicely into the friendly and easier to work with convex linear model framework. In our LinearRegression class, we implemented a linear predict function and a loss function that uses squared error, which is convex. By defining our predict and loss functions in these ways, we are then left with the empirical risk minimization problem of \\[\\hat{w}=\\underset{w}{\\arg\\min}\\sum_{i=1}^n(\\langle{\\mathbf{w}}, {\\mathbf{x}}_i\\rangle - y_i)^{2}=\\underset{w}{\\arg\\min}\\left\\lVert\\mathbf{Xw}-\\mathbf{y}\\right\\rVert_2^2.\\]\nTo solve this empirical risk minimization problem, we first take gradient with respect to \\(\\hat{w}\\), which results in \\[\\nabla L(w)=\\mathbf{X}^T(\\mathbf{Xw}-\\mathbf{y}).\\]\nTo continue solving this empirical risk minimization problem and find \\(\\hat{w}\\), we implemented two different fit methods in our LinearRegression class: fit_analytic and fit_gradient.\nIn fit_analytic, we use a formula involving matrix inversion that is obtained by using the condition \\(\\nabla L(w)=0\\). This ultimately results in \\[\\hat{w}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}\\] and the corresponding code np.linalg.inv(X_.T@X_)@X_.T@y for w.\nIn fit_gradient, we use gradient descent and update w until the change in loss is at its lowest. To compute our gradient descent, which is found through, \\[\\nabla L(w)=\\mathbf{X}^T(\\mathbf{Xw}-\\mathbf{y})\\] is a very expensive computation because \\(\\mathbf{X}^T\\mathbf{X}\\) has time complexity \\(O(np^2)\\) and \\(\\mathbf{X}^T\\mathbf{y}\\) has time complexity \\(O(np)\\). But, since the gradient doesn’t depend on our current w we precompute \\(\\mathbf{X}^T\\mathbf{X}\\) as \\({P}\\) and \\(\\mathbf{X}^T\\mathbf{y}\\) as \\({q}\\). This allows our w update to only take \\(O(p^2)\\) time."
  },
  {
    "objectID": "posts/linear-regression/index.html#demo",
    "href": "posts/linear-regression/index.html#demo",
    "title": "Implementing Linear Regression",
    "section": "Demo",
    "text": "Demo\nTo demonstrate that we’ve implemented the fit_analytic and fit_gradient functions of our LinearRegression class correctly, we can test it on a dataset that has 1 feature to easily visualize the problem and results. We first define relavant functions that will help us with this demo.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nNext, we create our training and validation data, each of which have 100 data points, and 1 feature.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nNow we can start fitting our model using our implemented LinearRegression class. We first test out our fit_analytic function.\n\nfrom linear_regression import LinearRegression\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train)\n\nprint(f\"Training score = {LR.score(pad(X_train), y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(pad(X_val), y_val).round(4)}\")\n\nTraining score = 0.6722\nValidation score = 0.573\n\n\nWhen we inspect our training and validation scores, we see that they may not be as high as the scores we would get on classification models in the past. This is because it all depends on the randomly generated data we get and how much noise is present in it. We can also inspect its estimated weight vector as shown below.\n\nLR.w\n\narray([0.90149111, 0.57773244])\n\n\nNext, we try testing out our fit_gradient function, and inspect its weight.\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, alpha = 0.01, max_iter = 1e3)\n\n\nLR2.w\n\narray([0.901481 , 0.5777378])\n\n\nWe see that its weight is pretty close to the weight calculated in our fit_analytic function. Since our fit_analytic keeps track of the scores of the current weight, we can also inspect its score_history.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nAs expected, we the score increases monotonically in each iteration. Based on our demo, it looks like our LinearRegression class is working adequately."
  },
  {
    "objectID": "posts/linear-regression/index.html#linear-regression-experiments",
    "href": "posts/linear-regression/index.html#linear-regression-experiments",
    "title": "Implementing Linear Regression",
    "section": "Linear Regression Experiments",
    "text": "Linear Regression Experiments\nNext, we want to explore what overparameterization can do to our model. To do so, we run an experiment where we increase p_features, the number of features used, but keep n_train, the number of training points, constant. We do this until p_features is all the way to n_train - 1 and inspect their change in training and validation scores.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR = LinearRegression()\n    LR.fit_analytic(X_train, y_train)\n    training_scores.append(LR.score(pad(X_train), y_train))\n    validation_scores.append(LR.score(pad(X_val), y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.yscale('log')\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\n\n\n\n\nWhen we inspect our evolution of training scores, we see that generally as our number of features increases, our training score also increases and eventually reaches perfect training classification. When we inspect our evolution of validation scores, however, we see that as our number of features increases, our validation score decreases and eventually shoots down to a very low validation score.\nThis evolution of the training and validation scores follows the pattern of overfitting. Here, as the number of features increase, we see a gap between the training and validation score get larger. Perhaps this tells us that the more features we use with least squares linear regression the more vulnerable we are to overfitting."
  },
  {
    "objectID": "posts/linear-regression/index.html#lasso-regularization-experiments",
    "href": "posts/linear-regression/index.html#lasso-regularization-experiments",
    "title": "Implementing Linear Regression",
    "section": "LASSO Regularization Experiments",
    "text": "LASSO Regularization Experiments\nLet’s try the same experiment with the LASSO algorithm as provided through the sklearn.linear_model. This algorithm has a modified loss function with a regularization term, which makes the entries of the weight vector \\(\\mathbf{w}\\) small. LASSO tends to force entries of the weight vector to be exactly zero, which may be desirable in our experiments where we are increasing the number of features to the number of data points.\nWe first experiment with our regularization set to \\({\\alpha} = 0.01\\).\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    L = Lasso(alpha = 0.01)\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.yscale('log')\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\n\n\n\n\nHere we see that as our number of features increased our training score still increasing to a close perfect classification, but the gap between our two scores is no longer as wide and our validation score is no longer as low as before.\nNow let’s set our regularization to \\({\\alpha} = 0.1\\).\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    L = Lasso(alpha = 0.1)\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.yscale('log')\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\n\n\n\n\nHere, when our regularization is set to an even higher value, we see the gap between our training and validation score get even smaller as our number of features increase.\nLet’s now see how the Lasso algorithm with a high regularization term can handle when our number of features goes past the number of data points we have.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train + 100):\n    L = Lasso(alpha = 0.1)\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    training_scores.append(L.score(X_train, y_train))\n    validation_scores.append(L.score(X_val, y_val))\n    p_features += 1\n    \nplt.plot(training_scores, label=\"training\")\nplt.plot(validation_scores, label=\"validation\")\n\nplt.yscale('log')\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\n\n\n\n\nBased on our graph of scores, we see that the validation score flucuates a little, but generally stays within the same range even as our number of features exceed our number of data points.\nAs we see through these experiments, the LASSO algorithm handles overparameterization better than our least square linear regression model."
  }
]