[
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Source Code: perceptron.py\nThe perceptron algorithm is a binary classification algorithm. It works to find a hyperplane that splits the given data into their respective labels. This machine learning algorithm, however, has the limitations that it only works on linearly separable data and data separated only into two groups.\n\n\nIn our Perceptron class, we implemented a fit(X, y) algorithm, which finds the variables of weights, w, that linearly separates X, a data set of observations and their features according to their labels y, such that an element in y is either 0 or 1. In this method, we first start with a random value for w. Then while we haven’t reached a 100% classification rate and have not exceeded the maximum number of steps, we continue to tweak our w depending on if it was able to correctly classify a random point i in our data set X.\nThe update of our weights, w, is seen on line 45 of our perceptron.py code. It is equivalent to the following math equation … \\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb{1}(\\tilde{y}_i {\\langle \\tilde{w}^{(t)}, \\tilde{x}_i\\rangle} < 0)\\tilde{y}_i \\tilde{x}_i\\]\nThis equation only updates w when our prediction for the label of \\(\\tilde{x}_i\\), the randomly selected data point, is incorrect. This is indicated through the \\(\\mathbb{1}(\\tilde{y}_i {\\langle \\tilde{w}^{(t)}, \\tilde{x}_i\\rangle} < 0)\\) portion of the equation, for it will evaulate to 0 when our w does classify the point correctly and the calcuations will not affect \\(\\tilde{w}^{(t+1)}\\), our w in the next iteration. When the indicator evaulates to 1, meaning our predication was incorrect, then our w is incremented by \\(\\tilde{y}_i \\tilde{x}_i\\). This moves our linear separator in the right direction, so that our randomly selected point could be classified correctly on the next iteration if it were selected.\n\n\n\nBefore we conduct any experiments, we need to import and define relevant extensions, classes, and functions.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron\n\nnp.random.seed(12345)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n\n\nHere is a data set of two distinct groups with 2 features.\n\nn = 100\np_features = 2\n\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(-1.75, -1.75), (1.75, 1.75)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAs shown in the graph, it looks like the points could be separated by a line so that all the purple points, whose label would be “0,” are on one side of the line and all the yellow points, whose label would be “1,” are on the other side of the line.\nWhen we run our Perceptron fit method on this data, we can inspect it’s history of scores. We see that over time the score flucuated as w was adjusted, but it eventually reaches 100% classification. This means that the fit method eventually found the variable of weights that would allow it to create a linear separator on the given data.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nWhen we then graph our line using the weights our Perceptron fit method found, we see that our algorithm was indeed able to separate the data into their respective labels.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nHere is another data set of two distinct groups with 2 features.\n\nn = 100\np_features = 2\n\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(0, 0), (0, 0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThis time, however, it does not look like we can separate the purple and yellow points from each other. Since the data points overlap, it does not look like a linear separator could separate the points into their respective labels.\nWhen looking at this set’s history of scores, we see that it too flucates. This time, however, we see that the score never reaches 1, in other words, it does not reach 100% classification. Instead, it eventually reaches the max number of times we’ll adjust w. While the Perceptron fit method still stores the last value of weights in the object, it communicates that it is not accurate as it gives the warning that the data was unable to converge.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n/Users/katiemacalintal/Desktop/Machine Learning/KatieMacalintal.github.io/posts/perceptron/perceptron.py:51: UserWarning: WARNING: Could not converge\n  warnings.warn(\"WARNING: Could not converge\")\n\n\n\n\n\nWhen we then graph our line using the weights the final iteration of our Perceptron fit method found, we see that, as expected, our algorithm was not able to separate the data into their expected labels. (Note: The scale of the axis may also look different)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nOur perceptron algorithm can also work in more than 2 dimensions. This time we’ve created a data set of two distinct groups with 7 features.\n\nn = 100\np_features = 7\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(-1, -1, -1, -1, -1, -1, -1), (1, 1, 1, 1, 1, 1, 1)])\n\nSince this is difficult to visualize, we will only be inspecting its history of scores. As we can see in the chart, our new data set is linearly separable as the score eventually reaches 1.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nWe can also alter this data set, so that the two groups overlap.\n\nn = 100\np_features = 7\nX, y = make_blobs(n_samples = n, n_features = p_features, centers = [(10, 10, 10, 10, 10, 10, 10), (10, 10, 10, 10, 10, 10, 10)])\n\nAs we can see by inspecting the history of scores and the warning that is thrown, our data set never converges.\n\np = Perceptron()\np.fit(X, y)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n/Users/katiemacalintal/Desktop/Machine Learning/KatieMacalintal.github.io/posts/perceptron/perceptron.py:51: UserWarning: WARNING: Could not converge\n  warnings.warn(\"WARNING: Could not converge\")\n\n\n\n\n\n\n\n\n\nIn the context of a single iteration of the perceptron algorithm update, I think the runtime complexity of our algorithm would depend on \\(p\\), the number of features. The operation that I think take the longest time would be \\({\\langle \\tilde{w}^{(t)}, \\tilde{x}_i\\rangle}\\), which I think takes \\(O(p)\\) time. There is other multiplication and addition that takes place in this equation, but since it happens consecutively it won’t matter at a larger scale. Thus, I think that runtime of the update would be \\(O(p)\\)."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implement first-order methods, including simple gradient descent, a momentum method, and stochastic gradient descent, comparing their performance for training logistic regression\n\n\n\n\n\n\nMar 2, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplement and explain the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets\n\n\n\n\n\n\nFeb 22, 2023\n\n\nKatie Macalintal\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Katie Macalintal\nMiddlebury College\nB.A. Computer Science\n2024"
  },
  {
    "objectID": "posts/optimization/index.html",
    "href": "posts/optimization/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Source Code: logistic-regression.py\nIn this blog post, we use logistic regression and gradient descent to efficiently find a hyperplane that can separate a binary classified data set with minimal loss, in other words minimize the empircal risk.\n\n\nIn our LogisticRegression class, we implemented a fit and fit_stochastic function that both take the data set X and their expected labels y.\nIn the fit function, we are looking for the weights w (which includes the bias term) such that it mimimizes our loss. In order to find this w, we use the gradient descent framework, which searches for this local minima. In this framework, we compute the gradient of our loss function: \\[\\ell(\\hat{y}, y)=-y\\log\\sigma(\\hat{y})-(1-y)\\log(1-\\sigma(\\hat{y})),\\] where \\(\\sigma\\) is the logistic sigmoid function and \\(\\hat{y}\\) is our prediction \\(\\langle w,x_i \\rangle\\). This loss function, known as the logistic loss function, is convient for us because it is strictly convex in it’s first argument meaning that our loss can have at most one minimum. The gradient of this loss function turns out to be: \\[\\nabla L(w)=(1/n)\\sum_{i=1}^n (\\sigma(\\hat{y_i})-y_i)x_i.\\] This gradient equation is implemented in the gradient function of the code like\nnp.mean(((self.sigmoid(y_) - y)[:,np.newaxis]) * X, axis = 0).\nThen, as stated in Theorem 2 of Optimization with Gradient Descent notes, because our gradent is a descent direction, we adjust our w by stepping in the direction of descent since we are looking for a w such that our loss is at the lowest it can be. We do this until we either reach the specified max_epochs or convergence. In this case, convergence is until the improvement in the our loss function is small enough in magnitiude.\nThe fit_stochastic function is very similar to the fit function, expect this time we don’t compute the complete gradient, we instead compute the gradient on a batch size.\n\n\n\nBefore we conduct any experiments, we need to import and define relevant extensions, classes, and functions.\n\nfrom logistic_regression import LogisticRegression # source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\ndef graph_fit_and_loss(X, y, LR):\n    fig, axarr = plt.subplots(1, 2)\n\n    axarr[0].scatter(X[:,0], X[:,1], c = y)\n    axarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.loss_history[-1]}\")\n\n    f1 = np.linspace(-3, 3, 101)\n\n    p = axarr[0].plot(f1, (- LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\n    axarr[1].plot(LR.loss_history)\n    axarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n    plt.tight_layout()\n\n\n\nWe first created a set of data with 2 features and see that their labels slightly overlaps with each other.\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can then fit this data using our fit method, which uses gradient descent, and fit_stochastic, which used batch gradient descent. When we give these fit methods a reasonable learning rate \\(\\alpha\\), then we would expect them to converge.\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 10000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\nAfter running these methods, we see that the data was able to converge in both cases. We see that they both reach the same loss, but the stochastic gradient descent reaches it in fewer iterations, meaning it iterated over all data points less times. This may be because it makes updates to w more frequently than regular gradient descent does, since it samples its w on smaller portions of data set X and updates as it goes.\n\n\n\n\nIt’s important that the learning rate \\(\\alpha\\) is relatively small. Before we set our alpha to 0.1, but if we use the same data and set it to a too high number we will see that we never converge in both regular gradient descent and stochastic gradient descent. If it’s too large, we might be updating w by too much such that it eventually skips over the minimum.\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 125, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 50, max_epochs = 1000, batch_size = 50)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\nLR.fit(X, y, alpha = 50, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\nWe see, however, that because stochastic gradient descent updates w more often depending on our batch_size, our fit_stochastic doesn’t converge with a high learning rate that our regular gradient descent method can converge with.\n\n\n\nNow, we will run some experiments on how the batch size influences convergence. We will do this with a larger data set that has 10 features.\n\n# Make the data\np_features = 11\nX, y = make_blobs(n_samples = 1000, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\ndef graph_loss(LR):\n    fig, axarr = plt.subplots(1, 1)\n\n    axarr.plot(LR.loss_history)\n    axarr.set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = f\"Loss = {LR.loss_history[-1]}\")\n    plt.tight_layout()\n\nIn the Disenroth, Faisal, and Soon reading, they observe that “Large mini-batch sizes will provide accurate estimates of the gradient, reducing the variance in the parameter update. … The reduction in variance leads to more stable convergence, but each graident calculation will be more expensive” (232) On the other hand, they observe that “small mini-batches are quick to estimate. If we keep the mini-batch size small, the noise in our graident estimate will allow us to get out of some bad local optima” (232). We’ll conduct experiments such that our batch_size decreases in size, and we’ll inspect their loss to see how Disenroth, Faisal, and Soon’s observations hold.\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 500)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 50)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 25)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 5)\n\ngraph_loss(LR)\n\n\n\n\nWe see that as our batch size gets smaller, our number of iterations over the all data points in X also gets smaller. This relates to why stochastic gradient descent uses fewer iterations as gradient descent. As our batch size gets larger, we do not updates w as frequently as we would have if it were smaller. However, stochastic gradient descent with large batch sizes"
  },
  {
    "objectID": "posts/optimization/momentum_experiments/index.html",
    "href": "posts/optimization/momentum_experiments/index.html",
    "title": "CSCI 0451 Blog",
    "section": "",
    "text": "In this function, we also can use momentum, which takes into account our previously taken step and allows us to continue moving along that direction if the update was good."
  },
  {
    "objectID": "posts/optimization/momentum_experiments/index.html#momentum",
    "href": "posts/optimization/momentum_experiments/index.html#momentum",
    "title": "CSCI 0451 Blog",
    "section": "Momentum",
    "text": "Momentum\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nLR = LogisticRegression()\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = False)\ngraph_fit_and_loss(X, y, LR)\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = True)\ngraph_fit_and_loss(X, y, LR)\n\nDifficult to tell that momentum makes a difference here.\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (0, 0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = False)\ngraph_fit_and_loss(X, y, LR)\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = True)\ngraph_fit_and_loss(X, y, LR)"
  },
  {
    "objectID": "posts/Untitled Folder/index.html",
    "href": "posts/Untitled Folder/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Source Code: logistic-regression.py\nIn this blog post, we use logistic regression and gradient descent to efficiently find a hyperplane that can separate a binary classified data set with minimal loss, in other words minimize the empircal risk.\n\n\nIn our LogisticRegression class, we implemented a fit and fit_stochastic function that both take the data set X and their expected labels y.\nIn the fit function, we are looking for the weights w (which includes the bias term) such that it mimimizes our loss. In order to find this w, we use the gradient descent framework, which searches for this local minima. In this framework, we compute the gradient of our loss function: \\[\\ell(\\hat{y}, y)=-y\\log\\sigma(\\hat{y})-(1-y)\\log(1-\\sigma(\\hat{y})),\\] where \\(\\sigma\\) is the logistic sigmoid function and \\(\\hat{y}\\) is our prediction \\(\\langle w,x_i \\rangle\\). This loss function, known as the logistic loss function, is convient for us because it is strictly convex in it’s first argument meaning that our loss can have at most one minimum. The gradient of this loss function turns out to be: \\[\\nabla L(w)=(1/n)\\sum_{i=1}^n (\\sigma(\\hat{y_i})-y_i)x_i.\\] This gradient equation is implemented in the gradient function of the code like\nnp.mean(((self.sigmoid(y_) - y)[:,np.newaxis]) * X, axis = 0).\nThen, as stated in Theorem 2 of Optimization with Gradient Descent notes, because our gradent is a descent direction, we adjust our w by stepping in the direction of descent since we are looking for a w such that our loss is at the lowest it can be. We do this until we either reach the specified max_epochs or convergence. In this case, convergence is until the improvement in the our loss function is small enough in magnitiude.\nThe fit_stochastic function is very similar to the fit function, expect this time we don’t compute the complete gradient, we instead compute the gradient on a batch size.\n\n\n\nBefore we conduct any experiments, we need to import and define relevant extensions, classes, and functions.\n\nfrom logistic_regression import LogisticRegression # source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\ndef graph_fit_and_loss(X, y, LR):\n    fig, axarr = plt.subplots(1, 2)\n\n    axarr[0].scatter(X[:,0], X[:,1], c = y)\n    axarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.loss_history[-1]}\")\n\n    f1 = np.linspace(-3, 3, 101)\n\n    p = axarr[0].plot(f1, (- LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\n    axarr[1].plot(LR.loss_history)\n    axarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n    plt.tight_layout()\n\n\n\nWe first created a set of data with 2 features and see that their labels slightly overlaps with each other.\n\n# Make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can then fit this data using our fit method, which uses gradient descent, and fit_stochastic, which used batch gradient descent. When we give these fit methods a reasonable learning rate \\(\\alpha\\), then we would expect them to converge.\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 10000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\nAfter running these methods, we see that the data was able to converge in both cases. We see that they both reach the same loss, but the stochastic gradient descent reaches it in fewer iterations, meaning it iterated over all data points less times. This may be because it makes updates to w more frequently than regular gradient descent does, since it samples its w on smaller portions of data set X and updates as it goes.\n\n\n\n\nIt’s important that the learning rate \\(\\alpha\\) is relatively small. Before we set our alpha to 0.1, but if we use the same data and set it to a too high number we will see that we never converge in both regular gradient descent and stochastic gradient descent. If it’s too large, we might be updating w by too much such that it eventually skips over the minimum.\n\n# Fit the model\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 125, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 50, max_epochs = 1000, batch_size = 50)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\n\nLR.fit(X, y, alpha = 50, max_epochs = 1000)\n\ngraph_fit_and_loss(X, y, LR)\n\n\n\n\nWe see, however, that because stochastic gradient descent updates w more often depending on our batch_size, our fit_stochastic doesn’t converge with a high learning rate that our regular gradient descent method can converge with.\n\n\n\nNow, we will run some experiments on how the batch size influences convergence. We will do this with a larger data set that has 10 features.\n\n# Make the data\np_features = 11\nX, y = make_blobs(n_samples = 1000, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\ndef graph_loss(LR):\n    fig, axarr = plt.subplots(1, 1)\n\n    axarr.plot(LR.loss_history)\n    axarr.set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = f\"Loss = {LR.loss_history[-1]}\")\n    plt.tight_layout()\n\nIn the Disenroth, Faisal, and Soon reading, they observe that “Large mini-batch sizes will provide accurate estimates of the gradient, reducing the variance in the parameter update. … The reduction in variance leads to more stable convergence, but each graident calculation will be more expensive” (232) On the other hand, they observe that “small mini-batches are quick to estimate. If we keep the mini-batch size small, the noise in our graident estimate will allow us to get out of some bad local optima” (232). We’ll conduct experiments such that our batch_size decreases in size, and we’ll inspect their loss to see how Disenroth, Faisal, and Soon’s observations hold.\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 500)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 50)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 25)\n\ngraph_loss(LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 5)\n\ngraph_loss(LR)\n\n\n\n\nWe see that as our batch size gets smaller, our number of iterations over the all data points in X also gets smaller. This relates to why stochastic gradient descent uses fewer iterations as gradient descent. As our batch size gets larger, we do not updates w as frequently as we would have if it were smaller. However, stochastic gradient descent with large batch sizes"
  }
]